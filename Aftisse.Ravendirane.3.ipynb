{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8f9cc0",
   "metadata": {},
   "source": [
    "[ ] Ajouter un commentaire le dataset 8 (mayo clinic dataset) : quoi que on ne utlise pas \n",
    "[ ] Ajouter une conclusion et r√©pondre aux questions pose en intro\n",
    "\n",
    "PHASE 1 \n",
    "- [ ] Choix et les d√©cisions qu'on avait fait \n",
    "- [X] Mise en place des tableaux en horizontal \n",
    "- [ ] Tableau recap pour les clean & strategie \n",
    "\n",
    "---\n",
    "M√©li:\n",
    "- [ ] Mise en place du tableau recap entre le cleaning et analyse pout tous les datasets\n",
    "- [ ] ligne de code pour ne pas prendre en compte les movements de la sourie\n",
    "- [ ] lien for the webscrapping de WHO\n",
    "- [X] tous les variables en communs entre les datasets \n",
    "    - datset 1 & 2 & 3 & 5 : age genre \n",
    "    - dataset 1 & 2 : BMI, CigPerDay (nbr de cigarette par jour) and smoking (0 ou 1)\n",
    "    - dataset 1 & 3 : total du cholesterol\n",
    "    - dataset 5: cdt m√©dical : diabetes cholesterol ... others\n",
    "- [X] pas de save un dataset_clean\n",
    "- [X] 2.1 ; \n",
    "    - [ ] a reverifier (decider est ce qu'on garde medical cdt et l intervalle ou pas)\n",
    "- [X] 2.3 ; \n",
    "    - [ ] petite interpr√©tation\n",
    "- [X] 3.1 ; \n",
    "    - [ ] A choisir quelle carte garder\n",
    "- [ ] 4.2 ; code pas correct\n",
    "- [ ] 5.2 ; \n",
    "- [ ] 5.3\n",
    "\n",
    "Gaya: \n",
    "- [X] Dataset 2: \n",
    "    - [X] enlever le physical activity\n",
    "- [X] TODO en fran√ßais\n",
    "- [X] Web scrapping WHO \n",
    "- [X] 2.2 ;\n",
    "- [ ] 3.2 ; \n",
    "    - [X] mise en page du legend ; en gris au lieu de blanc\n",
    "    - [X] A REFAIRE!!! carte pas bien fait les etats are missing\n",
    "    - [X] les importation en imporation section\n",
    "    - [ ] Choisir la carte √† garder\n",
    "- [X] 4.1 ;\n",
    "    - [X] color to change\n",
    "    - [X] Question--R√©ponses to add \n",
    "- [ ] 5.1\n",
    "\n",
    "---\n",
    "26/12\n",
    "- Web scrapping de Wikipedia : c'est le meme dataset que Dataset 4. \n",
    "\n",
    "27/12\n",
    "- Web scrapping de WHO : m√©lissa TODO the telechargement part. \n",
    "\n",
    "28/12 \n",
    "- tableau en horizontal \n",
    "- recap tableau A REFAIRE!!!! (Gaya)\n",
    "\n",
    "30/12\n",
    "- A discuter : du tableau (de le mettre avant le cleaning ensuite une section pour cleaning?)\n",
    "- A discuter : keep ww_kaggle dataset or webscrapped?\n",
    "- recap tableau du PHASE 1 done for dataset 2 and 5\n",
    "- 2.2 done\n",
    "- 3.2 done\n",
    "- 4.1 moti√© done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92001b70",
   "metadata": {},
   "source": [
    "<h1 style=\"padding:8px; margin:0px -20px; color:#FFF; background:#2E1A24;text-align:center;\">\n",
    "Quand les donn√©es racontent la vie : les secrets du sang\n",
    "</h1>\n",
    "\n",
    "- AFTISSE Melissa (< aftisse.melissa@etu.u-bordeaux.fr >)\n",
    "- RAVENDIRANE Gayathiri (< gayathiri.ravendirane@etu.u-bordeaux.fr >)\n",
    "\n",
    "Version : 3.0\n",
    "\n",
    "<h2 style=\"padding:10px; color:#FFF; background:#882255\">Introduction</h2>\n",
    "\n",
    "<h2 style=\"color:#882255\">Contexte et motivation</h2>\n",
    "\n",
    "<p>Le don de sang est un acte solidaire essentiel qui sauve des milliers de vies chaque ann√©e. Pourtant, derri√®re ce geste altruiste se cachent des enjeux de sant√© publique complexes : comprendre les facteurs de risque li√©s aux variations de pression art√©rielle chez les donneurs, optimiser la s√©curit√© des dons, et identifier les profils n√©cessitant un suivi particulier.</p>\n",
    "\n",
    "<p>Notre projet s'inscrit dans une d√©marche d'analyse de donn√©es biom√©dicales en croisant <strong>sept sources diversifi√©es</strong> : donn√©es cliniques (pression art√©rielle, valeurs sanguines), informations d√©mographiques (groupes sanguins par pays, comportements de don), donn√©es √©pid√©miologiques (pr√©valence de l'hypertension en Inde), et imagerie m√©dicale (cellules sanguines). Cette h√©t√©rog√©n√©it√© nous permet d'explorer le sujet sous plusieurs angles compl√©mentaires.</p>\n",
    "\n",
    "<h2 style=\"color:#882255\">Probl√©matiques et questions de recherche</h2>\n",
    "\n",
    "<p>√Ä travers ce travail, nous cherchons √† r√©pondre aux questions suivantes :</p>\n",
    "\n",
    "<strong>üìä Analyses descriptives et corr√©lations :</strong>\n",
    "<ul>\n",
    "  <li>Quels sont les profils d√©mographiques et cliniques des donneurs de sang selon les diff√©rentes bases de donn√©es ?</li>\n",
    "  <li>Existe-t-il des corr√©lations entre les param√®tres biologiques (h√©moglobine, glucose, cholest√©rol) et les variations de pression art√©rielle ?</li>\n",
    "  <li>Comment la distribution des groupes sanguins varie-t-elle g√©ographiquement, et peut-on identifier des zones avec des besoins sp√©cifiques en don ?</li>\n",
    "</ul>\n",
    "\n",
    "<strong>‚öïÔ∏è √âvaluation des risques :</strong>\n",
    "<ul>\n",
    "  <li>Quels sont les facteurs de risque d'hypotension ou d'hypertension post-don (√¢ge, sexe, fr√©quence des dons, conditions pr√©existantes) ?</li>\n",
    "  <li>Peut-on √©tablir des seuils d'alerte bas√©s sur les valeurs cliniques pour pr√©venir les complications lors du don ?</li>\n",
    "  <li>Quelles populations pr√©sentent une pr√©valence √©lev√©e d'hypertension et n√©cessiteraient une surveillance accrue ?</li>\n",
    "</ul>\n",
    "\n",
    "<strong>ü§ñ Mod√©lisation pr√©dictive :</strong>\n",
    "<ul>\n",
    "  <li>Est-il possible de pr√©dire la variation de pression art√©rielle apr√®s un don en fonction des caract√©ristiques du donneur ?</li>\n",
    "  <li>Peut-on construire un mod√®le de classification pour identifier les \"profils √† risque\" n√©cessitant une intervention pr√©ventive ?</li>\n",
    "  <li>L'analyse d'images de cellules sanguines par machine learning peut-elle apporter des informations compl√©mentaires sur l'√©tat de sant√© des donneurs ?</li>\n",
    "</ul>\n",
    "\n",
    "<strong>üåç Perspectives g√©ographiques et temporelles :</strong>\n",
    "<ul>\n",
    "  <li>Comment les comportements de don √©voluent-ils dans le temps (fr√©quence, fid√©lisation des donneurs) ?</li>\n",
    "  <li>Quelles sont les disparit√©s r√©gionales en termes de sant√© cardiovasculaire et de disponibilit√© des groupes sanguins ?</li>\n",
    "</ul>\n",
    "\n",
    "<p>Ces questions guideront notre analyse tout au long du projet, en mobilisant des techniques vari√©es : visualisations interactives, tests statistiques, mod√®les pr√©dictifs, et potentiellement des approches de deep learning pour l'analyse d'images m√©dicales.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18fd038",
   "metadata": {},
   "source": [
    "<h2 style=\"padding:10px; color:#FFF; background:#882255\">Importation du jeux de donn√©es</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71100dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library ---\n",
    "import os, math, re, glob, json, random, hashlib, textwrap, urllib.request\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict\n",
    "from collections import Counter\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib import robotparser\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# --- Data Manipulation & Analysis ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- Web Scraping & API ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# --- Interactive Widgets ---\n",
    "from ipywidgets import (\n",
    "    HBox, VBox, Label, IntSlider,\n",
    "    fixed, interact, interact_manual, interactive_output\n",
    ")\n",
    "\n",
    "# --- Machine Learning: Preprocessing & Clustering ---\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- Machine Learning: Models & Evaluation ---\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --- Statistics ---\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e206a58",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">1‚ÄìHyperRisk_kaggle</h3>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/datasets/khan1803115/hypertension-risk-model-main \" style=\"color :#882255; font-style: italic \"> Lien du jeu de donn√©es </a>\n",
    "\n",
    "- (4240, 13)\n",
    "- **Type de donn√©es pr√©sentes :** num√©rique\n",
    "- **Format du fichier :** `.csv`\n",
    "- **Points cl√©s :** variables li√©es √† l‚Äô**hypertension** et facteurs de risque (√¢ge, mesures cliniques). Utile pour cr√©er des indicateurs de risque, segmenter des patients et entra√Æner des mod√®les pr√©dictifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfca3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperRisk_kaggle = pd.read_csv(\"https://gayathiri-ravendirane.emi.u-bordeaux.fr/DATA/HyperRisk_kaggle.csv\")\n",
    "print(HyperRisk_kaggle.shape)\n",
    "HyperRisk_kaggle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32d8ab",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">2‚ÄìBloodPressure_kaggle</h3>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/datasets/jayaprakashpondy/blood-pressure \" style=\"color :#882255; font-style: italic \"> Lien du jeu de donn√©es </a>\n",
    "\n",
    "- (2000, 15)\n",
    "- **Type de donn√©es pr√©sentes :** num√©rique \n",
    "- **Format du fichier :** `.csv`\n",
    "- **Points cl√©s :** mesures de pression art√©rielle (systolique/diastolique) et m√©tadonn√©es (p. ex. √¢ge, sexe, habitudes). Utile pour la distribution des BP, la d√©finition de seuils (normale/√©lev√©e/hypertension) et l‚Äô√©tude de corr√©lations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "BloodPressure_kaggle = pd.read_csv(\"https://gayathiri-ravendirane.emi.u-bordeaux.fr/DATA/BloodPressure_kaggle.csv\")\n",
    "print(BloodPressure_kaggle.shape)\n",
    "BloodPressure_kaggle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7395c11",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">3‚ÄìBloodValues_kaggle</h3>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/datasets/ilhanuysal/bloodvalues  \" style=\"color :#882255; font-style: italic \"> Lien du jeu de donn√©es </a>\n",
    "\n",
    "- (4598, 14)\n",
    "- **Type de donn√©es pr√©sentes :** num√©rique\n",
    "- **Format du fichier :** `.csv`\n",
    "- **Points cl√©s :** param√®tres biologiques (valeurs sanguines) permettant des analyses de profils, la d√©tection d‚Äôanomalies et le croisement avec BP/hypotension pour enrichir les features des mod√®les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BloodValues_kaggle = pd.read_excel(\"https://gayathiri-ravendirane.emi.u-bordeaux.fr/DATA/BloodValues_kaggle.xlsx\")\n",
    "print(BloodValues_kaggle.shape)\n",
    "BloodValues_kaggle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf24476",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">4‚ÄìBloodGroupWW_kaggle (distribution des groupes sanguins par pays)</h3>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/datasets/shuvokumarbasak4004/global-blood-group-distribution-worldwide-dataset  \" style=\"color :#882255; font-style: italic \"> Lien du jeu de donn√©es </a>\n",
    "\n",
    "- (126, 10)\n",
    "- **Type de donn√©es pr√©sentes :** cat√©gorielle, num√©rique\n",
    "- **Format du fichier :** `.csv`\n",
    "- **Points cl√©s :** r√©partition des groupes sanguins (A, B, AB, O, Rh) par pays. Utile pour des cartes/charts comparatifs, analyses g√©ographiques et mise en relation disponibilit√©/besoins m√©dicaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de42b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BloodGroupWW_kaggle = pd.read_csv(\"https://gayathiri-ravendirane.emi.u-bordeaux.fr/DATA/BloodGroupWW_kaggle/cleaned_blood_type_distribution_by_country.csv\")\n",
    "print(BloodGroupWW_kaggle.shape)\n",
    "BloodGroupWW_kaggle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5076f6b",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">5‚ÄìBloodDonation_kaggle</h3>\n",
    "\n",
    "<a href=\"https://www.kaggle.com/datasets/kundanbedmutha/blood-donation-portal-dataset  \" style=\"color :#882255; font-style: italic \"> Lien du jeu de donn√©es </a>\n",
    "\n",
    "- (4598, 14)\n",
    "- **Type de donn√©es pr√©sentes :** cat√©gorielle, num√©rique et dates\n",
    "- **Format du fichier :** `.csv`\n",
    "- **Points cl√©s :** ce jeu de donn√©es contient des informations d√©mographiques, des conditions m√©dicales et des informations d‚Äô√©ligibilit√©. Il ouvre la voie √† des analyses descriptives des donneurs en explorant les variables (age, sexe, type de sang, cdt medicales...), √† des mod√©lisations pr√©dictives des dons (afin d'optimiser les campagnes de relance par exemple)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a334ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BloodDonation_kaggle = pd.read_csv(\"https://melissa-aftisse.emi.u-bordeaux.fr/DATA/blood_donation.csv\")\n",
    "print(BloodDonation_kaggle.shape)\n",
    "BloodDonation_kaggle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72d893",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">6‚ÄìBloodIndia_gouv</h3>\n",
    "\n",
    "<a href=\"https://www.data.gov.in/resource/stateut-wise-prevalence-hypertension-blood-glucose-obesity-among-adults-and-overweight\" style=\"color :#882255; font-style: italic \"> Lien du jeu de donn√©es </a>\n",
    "\n",
    "- (36, 8)\n",
    "- **Type de donn√©es pr√©sentes :** num√©rique (pourcentages), cat√©gorielle (√âtats/Territoires de l'Inde)\n",
    "- **Format du fichier :** `.json`\n",
    "- **Points cl√©s :** donn√©es √©pid√©miologiques par √âtat/Territoire indien concernant la pr√©valence de l'**hypertension**, du diab√®te (glucose sanguin √©lev√©), de l'ob√©sit√© et du surpoids chez les adultes. Utile pour des analyses g√©ographiques comparatives, l'identification de zones √† risque √©lev√©, et l'√©tude des corr√©lations entre diff√©rents indicateurs de sant√©. Permet de contextualiser les probl√©matiques de sant√© li√©es au sang dans un cadre g√©ographique sp√©cifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2ec34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Define the URL\n",
    "url = \"https://gayathiri-ravendirane.emi.u-bordeaux.fr/DATA/BloodIndia_gouv.json\"\n",
    "\n",
    "try:\n",
    "    # 2. Fetch the data using requests\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "    # 3. Parse the JSON content into a Python dictionary\n",
    "    data_dict = response.json()\n",
    "\n",
    "    # 4. Manually extract column names and data rows\n",
    "    # The column names are under the 'label' key in the 'fields' list\n",
    "    column_names = [field['label'] for field in data_dict['fields']]\n",
    "    \n",
    "    # The actual data is under the 'data' key\n",
    "    data_rows = data_dict['data']\n",
    "\n",
    "    # 5. Create the DataFrame\n",
    "    BloodIndia_gouv = pd.DataFrame(data_rows, columns=column_names)\n",
    "\n",
    "    # Display the result\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(BloodIndia_gouv.shape)\n",
    "    print(BloodIndia_gouv.head())\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching the URL: {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error parsing JSON structure. Missing expected key: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d5d85",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">7‚ÄìBCCD_github</h3>\n",
    "\n",
    "<a href=\"https://github.com/akshaylamba/all_CELL_data.git\" style=\"color :#882255; font-style: italic \"> Lien du jeu de donn√©es </a>\n",
    "\n",
    "- (410+ images, annotations XML associ√©es)\n",
    "- **Type de donn√©es pr√©sentes :** images microscopiques de cellules sanguines avec annotations XML (coordonn√©es de bounding boxes)\n",
    "- **Format du fichier :** `.jpg` (images) et `.xml` (annotations)\n",
    "- **Points cl√©s :** collection d'images de frottis sanguins color√©s montrant diff√©rents types de cellules (globules rouges, globules blancs, plaquettes). Chaque image est accompagn√©e d'un fichier XML contenant les annotations pour la d√©tection d'objets. Id√©al pour entra√Æner des mod√®les de **computer vision** (d√©tection d'objets, classification de cellules) et pour l'apprentissage automatique appliqu√© √† l'analyse d'images m√©dicales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c433eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for blood images\n",
    "base_url = \"https://gayathiri-ravendirane.emi.u-bordeaux.fr/DATA/BCCD_github/\"\n",
    "\n",
    "# List of first 5 image filenames\n",
    "image_files = [\n",
    "    \"BloodImage_00000.jpg\",\n",
    "    \"BloodImage_00001.jpg\",\n",
    "    \"BloodImage_00002.jpg\",\n",
    "    \"BloodImage_00003.jpg\",\n",
    "    \"BloodImage_00004.jpg\"\n",
    "]\n",
    "\n",
    "# Create a figure with a 1x5 grid\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for idx, img_file in enumerate(image_files):\n",
    "    img_url = base_url + img_file\n",
    "    response = requests.get(img_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(img_file)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5708e",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">8‚ÄìMayo Clinic (pages publiques de sant√©)</h3>\n",
    "\n",
    "<ul>\n",
    "  <li><strong>Objectif :</strong> extraire des contenus <em>li√©s au sang</em> : tests sanguins (CBC, h√©moglobine, h√©matocrite, ferritine, frottis sanguin, coagulation), composantes et pathologies (an√©mie, leuc√©mie, h√©mophilie, thrombocytop√©nie, dr√©panocytose...).</li>\n",
    "  <li><strong>Strat√©gie :</strong> pages HTML statiques uniquement, respect de <em>robots.txt</em>; index par lettre + <em>liste de graines</em> (URLs cibl√©es) pour garantir une base minimale; filtrage pour ne conserver que les pages explicitement <em>blood-related</em>.</li>\n",
    "  <li><strong>Donn√©es collect√©es :</strong>\n",
    "    <ul>\n",
    "      <li><em>Tests</em> : titre, URL, description (Overview), sections cl√©s (Symptoms / Risk factors / Results / Why it's done), indices d‚Äôunit√©s et de plages (d√©tect√©s dans textes/tableaux).</li>\n",
    "      <li><em>Maladies</em> : titre, URL, description, <em>Symptoms</em>, <em>Risk factors</em>.</li>\n",
    "      <li><em>Relations</em> : appariements tests ‚Üî maladies via TF‚ÄëIDF + matching de nom; clustering simple des maladies par impact sanguin.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li><strong>Sorties :</strong> fichiers CSV sous <code>DATA/scraped_mayo/</code> : <code>mayo_tests.csv</code>, <code>mayo_diseases.csv</code>, <code>mayo_relations.csv</code>, <code>mayo_disease_clusters.csv</code>.</li>\n",
    "  <li><strong>Limites :</strong> les slugs/URLs peuvent changer; certaines graines peuvent √©chouer (404) ‚Äî le code journalise ces cas et continue; la d√©tection d‚Äôunit√©s/plages est heuristique; contenu public uniquement.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1923db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Mayo Clinic scraping helpers (ethical, low-rate)\n",
    "\n",
    "# BASE = \"https://www.mayoclinic.org\"\n",
    "# ALLOWED_PATHS = [\"/tests-procedures/\", \"/diseases-conditions/\"]\n",
    "# HEADERS = {\n",
    "#     \"User-Agent\": \"UBx-EDU BloodDataProject (educational; contact: students@u-bordeaux.fr)\",\n",
    "#     \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "# }\n",
    "\n",
    "# # Respect robots.txt\n",
    "# _rp = robotparser.RobotFileParser()\n",
    "# _rp.set_url(urljoin(BASE, \"/robots.txt\"))\n",
    "# try:\n",
    "#     _rp.read()\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# def allowed(url: str) -> bool:\n",
    "#     try:\n",
    "#         p = urlparse(url)\n",
    "#         if not p.netloc:\n",
    "#             url = urljoin(BASE, url)\n",
    "#             p = urlparse(url)\n",
    "#         # Only target our whitelisted sections\n",
    "#         if not any(p.path.startswith(ap) for ap in ALLOWED_PATHS):\n",
    "#             return False\n",
    "#         return _rp.can_fetch(HEADERS[\"User-Agent\"], url)\n",
    "#     except Exception:\n",
    "#         return False\n",
    "\n",
    "# SESSION = requests.Session()\n",
    "# SESSION.headers.update(HEADERS)\n",
    "\n",
    "# RATE_SECONDS = 2.0  # low request rate\n",
    "# _last_ts = 0.0\n",
    "\n",
    "# def fetch(url: str) -> tuple[BeautifulSoup, list[pd.DataFrame]]:\n",
    "#     global _last_ts\n",
    "#     if not allowed(url):\n",
    "#         raise RuntimeError(f\"Blocked by robots.txt or outside allowed paths: {url}\")\n",
    "#     # rate limit\n",
    "#     now = time.time()\n",
    "#     wait = RATE_SECONDS - (now - _last_ts)\n",
    "#     if wait > 0:\n",
    "#         time.sleep(wait)\n",
    "#     _last_ts = time.time()\n",
    "\n",
    "#     r = SESSION.get(url, timeout=20)\n",
    "#     r.raise_for_status()\n",
    "#     soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "#     tables = []\n",
    "#     try:\n",
    "#         # read_html can raise a variety of parser errors; ignore if so\n",
    "#         tables = pd.read_html(r.text)\n",
    "#     except Exception:\n",
    "#         tables = []\n",
    "#     return soup, tables\n",
    "\n",
    "# # Generic utilities\n",
    "# WS = re.compile(r\"\\s+\")\n",
    "# UNIT_RE = re.compile(r\"(g/dL|mg/dL|mmol/L|%|x?10\\^?9/L|mcg/L|ng/mL|fL|pg|10\\^\\d+/L|cells/\\u00b5L)\")\n",
    "# RANGE_RE = re.compile(r\"(\\d+(?:\\.\\d+)?)\\s*(?:to|-|‚Äì)\\s*(\\d+(?:\\.\\d+)?)\\s*(g/dL|mg/dL|mmol/L|%|fL|pg|10\\^\\d+/L|cells/\\u00b5L)?\", re.I)\n",
    "\n",
    "\n",
    "# def clean_text(s: str) -> str:\n",
    "#     return WS.sub(\" \", (s or \"\").strip())\n",
    "\n",
    "\n",
    "# def extract_main_text(soup: BeautifulSoup) -> str:\n",
    "#     # Try article or main content\n",
    "#     parts = []\n",
    "#     if (h1 := soup.find(\"h1\")):\n",
    "#         parts.append(clean_text(h1.get_text(\" \")))\n",
    "#     # Grab paragraphs in main/article regions\n",
    "#     for sel in [\"article\", \"main\", \"div.content\", \"div#main-content\", \"div.container\"]:\n",
    "#         node = soup.select_one(sel)\n",
    "#         if node:\n",
    "#             for p in node.find_all([\"p\", \"li\"]):\n",
    "#                 txt = clean_text(p.get_text(\" \"))\n",
    "#                 if len(txt) > 0:\n",
    "#                     parts.append(txt)\n",
    "#             break\n",
    "#     if not parts:\n",
    "#         # fallback: all paragraphs\n",
    "#         for p in soup.find_all(\"p\"):\n",
    "#             parts.append(clean_text(p.get_text(\" \")))\n",
    "#     return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "# def extract_sections(soup: BeautifulSoup, names=(\"Overview\", \"Symptoms\", \"Risk factors\", \"When to see a doctor\", \"Results\", \"Why it's done\", \"What you can expect\")):\n",
    "#     sections = {}\n",
    "#     headings = soup.find_all([\"h2\", \"h3\"])\n",
    "#     for h in headings:\n",
    "#         title = clean_text(h.get_text(\" \"))\n",
    "#         for target in names:\n",
    "#             if target.lower() in title.lower():\n",
    "#                 # collect sibling paragraphs until next heading\n",
    "#                 texts = []\n",
    "#                 for sib in h.find_all_next():\n",
    "#                     if sib.name in (\"h2\", \"h3\"):\n",
    "#                         break\n",
    "#                     if sib.name in (\"p\", \"li\"):\n",
    "#                         texts.append(clean_text(sib.get_text(\" \")))\n",
    "#                 sections[target] = \"\\n\".join(texts)\n",
    "#     return sections\n",
    "\n",
    "\n",
    "# def extract_units_and_ranges(text: str) -> dict:\n",
    "#     units = sorted(set(m.group(1) for m in UNIT_RE.finditer(text)))\n",
    "#     ranges = [m.groups() for m in RANGE_RE.finditer(text)]\n",
    "#     return {\"units\": units, \"ranges\": ranges}\n",
    "\n",
    "\n",
    "# def index_links(base_path: str, letter: str) -> list[tuple[str, str]]:\n",
    "#     url = f\"{BASE}{base_path}index?letter={letter.upper()}\"\n",
    "#     soup, _ = fetch(url)\n",
    "#     links = []\n",
    "#     for a in soup.select(\"a\"):\n",
    "#         href = a.get(\"href\")\n",
    "#         if not href:\n",
    "#             continue\n",
    "#         if href.startswith(base_path) and allowed(urljoin(BASE, href)):\n",
    "#             title = clean_text(a.get_text(\" \"))\n",
    "#             links.append((urljoin(BASE, href), title))\n",
    "#     # de-dup\n",
    "#     seen = set()\n",
    "#     out = []\n",
    "#     for u, t in links:\n",
    "#         if u not in seen:\n",
    "#             out.append((u, t))\n",
    "#             seen.add(u)\n",
    "#     return out\n",
    "\n",
    "# TEST_KEYWORDS = [\n",
    "#     \"cbc\", \"complete blood count\", \"hemoglobin\", \"hematocrit\", \"glucose\", \"blood sugar\", \"cholesterol\", \"lipid\",\n",
    "#     \"iron\", \"ferritin\", \"transferrin\", \"platelet\", \"prothrombin\", \"inr\", \"pt\", \"aptt\", \"blood smear\",\n",
    "# ]\n",
    "\n",
    "# DISEASE_KEYWORDS = [\n",
    "#     \"anemia\", \"leukemia\", \"thrombocytopenia\", \"hemophilia\", \"von willebrand\", \"polycythemia\", \"thrombosis\",\n",
    "#     \"deep vein thrombosis\", \"dvt\", \"pulmonary embolism\", \"clot\", \"clotting\", \"sickle cell\", \"thalassemia\",\n",
    "#     \"hemolytic anemia\", \"aplastic anemia\", \"myeloma\",\n",
    "# ]\n",
    "\n",
    "# LETTERS_TESTS = list({k[0].upper() for k in TEST_KEYWORDS if k}) + [\"C\", \"H\", \"G\", \"I\", \"P\", \"F\", \"T\", \"S\", \"L\"]\n",
    "# LETTERS_DISEASES = list({k[0].upper() for k in DISEASE_KEYWORDS if k}) + [\"A\", \"L\", \"T\", \"H\", \"P\", \"S\", \"V\", \"D\"]\n",
    "\n",
    "# print(\"Robots allowed for tests root?\", allowed(\"/tests-procedures/\"))\n",
    "# print(\"Robots allowed for diseases root?\", allowed(\"/diseases-conditions/\"))\n",
    "# print(\"Helper utilities ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa91051",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Seed known Mayo Clinic pages to ensure initial coverage\n",
    "# test_seed = [\n",
    "#     \"/tests-procedures/complete-blood-count/about/pac-20384919\",\n",
    "#     \"/tests-procedures/hemoglobin-test/about/pac-20385075\",\n",
    "#     \"/tests-procedures/hematocrit-test/about/pac-20384728\",\n",
    "#     \"/tests-procedures/ferritin-test/about/pac-20384928\",\n",
    "#     \"/tests-procedures/blood-smear/about/pac-20393004\",\n",
    "#     # Coagulation (if available)\n",
    "#     \"/tests-procedures/prothrombin-time/about/pac-20385287\",\n",
    "# ]\n",
    "\n",
    "# disease_seed = [\n",
    "#     \"/diseases-conditions/anemia/symptoms-causes/syc-20351360\",\n",
    "#     \"/diseases-conditions/leukemia/symptoms-causes/syc-20374373\",\n",
    "#     \"/diseases-conditions/thrombocytopenia/symptoms-causes/syc-20378293\",\n",
    "#     \"/diseases-conditions/hemophilia/symptoms-causes/syc-20373327\",\n",
    "#     \"/diseases-conditions/sickle-cell-anemia/symptoms-causes/syc-20355876\",\n",
    "#     \"/diseases-conditions/thalassemia/symptoms-causes/syc-20354995\",\n",
    "#     \"/diseases-conditions/aplastic-anemia/symptoms-causes/syc-20355015\",\n",
    "#     \"/diseases-conditions/hemolytic-anemia/symptoms-causes/syc-20352360\",\n",
    "#     \"/diseases-conditions/multiple-myeloma/symptoms-causes/syc-20353378\",\n",
    "# ]\n",
    "\n",
    "# def collect_pages(paths, kind=\"test\"):\n",
    "#     rows = []\n",
    "#     for p in paths:\n",
    "#         url = urljoin(BASE, p)\n",
    "#         try:\n",
    "#             soup, tables = fetch(url)\n",
    "#             title = clean_text(soup.find(\"h1\").get_text(\" \")) if soup.find(\"h1\") else p.split(\"/\")[-1]\n",
    "#             text = extract_main_text(soup)\n",
    "#             secs = extract_sections(soup)\n",
    "#             meta = extract_units_and_ranges(\"\\n\".join([text] + list(secs.values()))) if kind==\"test\" else {}\n",
    "#             row = {\n",
    "#                 \"url\": url,\n",
    "#                 \"title\": title,\n",
    "#                 \"description\": text[:1200],\n",
    "#             }\n",
    "#             if kind==\"test\":\n",
    "#                 row.update({\n",
    "#                     \"sections\": json.dumps(secs, ensure_ascii=False),\n",
    "#                     \"reference_tables_count\": len(tables),\n",
    "#                     \"units_candidates\": \", \".join(meta.get(\"units\", [])),\n",
    "#                     \"ranges_found\": len(meta.get(\"ranges\", [])),\n",
    "#                 })\n",
    "#             else:\n",
    "#                 row.update({\n",
    "#                     \"symptoms\": secs.get(\"Symptoms\", \"\")[:1000],\n",
    "#                     \"risk_factors\": secs.get(\"Risk factors\", \"\")[:1000],\n",
    "#                     \"tables\": len(tables),\n",
    "#                 })\n",
    "#             rows.append(row)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Seed fetch fail: {url} -> {e}\")\n",
    "#     return rows\n",
    "\n",
    "# # Build/augment dataframes from seeds\n",
    "# seed_tests_df = pd.DataFrame(collect_pages(test_seed, kind=\"test\"))\n",
    "# seed_diseases_df = pd.DataFrame(collect_pages(disease_seed, kind=\"disease\"))\n",
    "\n",
    "# # Merge with previously scraped frames if any\n",
    "# if 'tests_df' in globals() and not tests_BloodPressure_kaggle.empty:\n",
    "#     tests_BloodPressure_kaggle = pd.concat([tests_df, seed_tests_df], ignore_index=True).drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "# else:\n",
    "#     tests_df = seed_tests_df.copy()\n",
    "\n",
    "# if 'diseases_df' in globals() and not diseases_df.empty:\n",
    "#     diseases_df = pd.concat([diseases_df, seed_diseases_df], ignore_index=True).drop_duplicates(subset=[\"url\"]).reset_index(drop=True)\n",
    "# else:\n",
    "#     diseases_df = seed_diseases_df.copy()\n",
    "\n",
    "# # Filter out non-blood-related tests\n",
    "# BLOOD_TERMS = [\n",
    "#     \"blood\", \"hemoglobin\", \"hematocrit\", \"platelet\", \"cbc\", \"ferritin\", \"iron\", \"smear\",\n",
    "#     \"coagulation\", \"prothrombin\", \"inr\", \"aptt\", \"clot\", \"cholesterol\", \"lipid\"\n",
    "# ]\n",
    "\n",
    "# def is_blood_related(row):\n",
    "#     txt = f\"{row.get('title','')} {row.get('description','')}\".lower()\n",
    "#     return any(term in txt for term in BLOOD_TERMS)\n",
    "\n",
    "# if not tests_df.empty:\n",
    "#     tests_df = tests_df[tests_df.apply(is_blood_related, axis=1)].reset_index(drop=True)\n",
    "\n",
    "# print(\"tests_df shape after seed & filter:\", tests_df.shape)\n",
    "# print(\"diseases_df shape after seed:\", diseases_df.shape)\n",
    "# display(tests_df.head(5) if not tests_df.empty else tests_df)\n",
    "# display(diseases_df.head(5) if not diseases_df.empty else diseases_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "mayo_diseases = pd.read_csv(\"https://gayathiri-ravendirane.emi.u-bordeaux.fr/DATA/ScrapedMayoClinic/mayo_diseases.csv\")\n",
    "print(mayo_diseases.shape)\n",
    "mayo_diseases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mayo_tests = pd.read_csv(\"https://gayathiri-ravendirane.emi.u-bordeaux.fr/DATA/ScrapedMayoClinic/mayo_tests.csv\")\n",
    "print(mayo_tests.shape)\n",
    "mayo_tests.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0bec3",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">9‚ÄìWorld Health Organisation--WHO (pages publiques de sant√©)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE = \"https://ghoapi.azureedge.net/api\"\n",
    "# session = requests.Session()\n",
    "# session.headers.update({\"Accept\": \"application/json\"})\n",
    "\n",
    "# def gho_get(path: str, params: Optional[Dict]=None) -> Dict:\n",
    "#     url = f\"{BASE}/{path.strip('/')}\"\n",
    "#     r = session.get(url, params=params, timeout=30)\n",
    "#     r.raise_for_status()\n",
    "#     return r.json()\n",
    "\n",
    "# def list_indicators() -> pd.DataFrame:\n",
    "#     data = gho_get(\"Indicator\")\n",
    "#     return pd.DataFrame(data.get(\"value\", []))\n",
    "\n",
    "# def find_indicators(substrs: List[str]) -> pd.DataFrame:\n",
    "#     df = list_indicators()\n",
    "#     mask = pd.Series(True, index=df.index)\n",
    "#     for s in substrs:\n",
    "#         s_low = s.lower()\n",
    "#         mask &= df[\"IndicatorName\"].str.lower().str.contains(s_low, na=False)\n",
    "#     return df.loc[mask, [\"IndicatorCode\", \"IndicatorName\"]].drop_duplicates()\n",
    "\n",
    "# def fetch_indicator_values(code: str) -> pd.DataFrame:\n",
    "#     # Try direct endpoint first (common for many indicators)\n",
    "#     try:\n",
    "#         data = gho_get(code)\n",
    "#         vals = pd.DataFrame(data.get(\"value\", []))\n",
    "#         if not vals.empty:\n",
    "#             vals[\"IndicatorCode\"] = code\n",
    "#             return vals\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     # Fallback: Value entity filtered by IndicatorCode\n",
    "#     try:\n",
    "#         data = gho_get(\"Value\", params={\"$filter\": f\"IndicatorCode eq '{code}'\"})\n",
    "#         vals = pd.DataFrame(data.get(\"value\", []))\n",
    "#         if not vals.empty:\n",
    "#             vals[\"IndicatorCode\"] = code\n",
    "#             return vals\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     return pd.DataFrame()\n",
    "\n",
    "# def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     if df.empty:\n",
    "#         return df\n",
    "#     cols = df.columns\n",
    "#     # Prefer NumericValue if present, else Value\n",
    "#     val_col = \"NumericValue\" if \"NumericValue\" in cols else (\"Value\" if \"Value\" in cols else None)\n",
    "#     out = df.copy()\n",
    "#     if val_col:\n",
    "#         out = out.rename(columns={val_col: \"value\"})\n",
    "#     # Standard country/year columns used by GHO\n",
    "#     for c_old, c_new in [(\"SpatialDim\", \"country\"), (\"TimeDim\", \"year\")]:\n",
    "#         if c_old in out.columns:\n",
    "#             out = out.rename(columns={c_old: c_new})\n",
    "#     # Keep key columns\n",
    "#     keep = [c for c in [\"country\", \"year\", \"IndicatorCode\", \"value\"] if c in out.columns]\n",
    "#     return out[keep].dropna(subset=[\"country\", \"year\"]).reset_index(drop=True)\n",
    "\n",
    "# # Discover relevant indicators\n",
    "# ind_blood_safety = find_indicators([\"blood\", \"safety\"])\n",
    "# ind_blood_group = find_indicators([\"blood\", \"group\"])\n",
    "# ind_raised_bp = find_indicators([\"raised\", \"blood\", \"pressure\"])\n",
    "\n",
    "# # Helper: choose a code by keywords in IndicatorName\n",
    "# def pick_code(df: pd.DataFrame, keywords: List[str]) -> Optional[str]:\n",
    "#     if df is None or df.empty:\n",
    "#         return None\n",
    "#     m = pd.Series(False, index=df.index)\n",
    "#     for kw in keywords:\n",
    "#         m |= df[\"IndicatorName\"].str.lower().str.contains(kw.lower(), na=False)\n",
    "#     subset = df.loc[m] if m.any() else df\n",
    "#     return subset.iloc[0][\"IndicatorCode\"] if not subset.empty else None\n",
    "\n",
    "# # Explore additional blood-related indicators by keywords\n",
    "# ind_blood_any = find_indicators([\"blood\"])  # broad search\n",
    "# ind_donation = find_indicators([\"donation\"])  # donation-related\n",
    "# ind_transfusion = find_indicators([\"blood\", \"transfusion\"])  # transfusion-related\n",
    "\n",
    "# # Select codes including donation/transfusion proxies for blood safety\n",
    "# code_bp = pick_code(ind_raised_bp, [\"raised blood pressure\"]) or pick_code(ind_blood_any, [\"raised blood pressure\"]) \n",
    "# code_donation = pick_code(ind_donation, [\"donation\", \"donor\", \"blood donation\"]) \n",
    "# code_transfusion = pick_code(ind_transfusion, [\"transfusion\"]) \n",
    "# selected = {\"RaisedBloodPressure\": code_bp, \"Donation\": code_donation, \"Transfusion\": code_transfusion}\n",
    "\n",
    "# # Fetch and merge selected indicator data (no prints, no file writes)\n",
    "# codes = [c for c in [code_bp, code_donation, code_transfusion] if c]\n",
    "# frames = []\n",
    "# for c in codes:\n",
    "#     df_raw = fetch_indicator_values(c)\n",
    "#     df_norm = normalize_df(df_raw)\n",
    "#     frames.append(df_norm)\n",
    "# merged_long = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=[\"country\",\"year\",\"IndicatorCode\",\"value\"]) \n",
    "# merged_wide = merged_long.pivot_table(index=[\"country\",\"year\"], columns=\"IndicatorCode\", values=\"value\", aggfunc=\"mean\").reset_index()\n",
    "\n",
    "# # Build a small mapping from IndicatorCode to IndicatorName (no prints, no file writes)\n",
    "# ind_df = list_indicators()\n",
    "# code_cols = [c for c in merged_wide.columns if c not in (\"country\", \"year\")]\n",
    "# map_rows = ind_df[ind_df[\"IndicatorCode\"].isin(code_cols)][[\"IndicatorCode\", \"IndicatorName\"]].drop_duplicates()\n",
    "# code_to_name = dict(zip(map_rows[\"IndicatorCode\"], map_rows[\"IndicatorName\"]))\n",
    "\n",
    "# # Create a human-readable version of the merged table (no prints, no file writes)\n",
    "# rename_map = {code: f\"{code_to_name.get(code, code)} [{code}]\" for code in code_cols}\n",
    "# merged_wide_named = merged_wide.rename(columns=rename_map)\n",
    "\n",
    "# # The final DataFrames are:\n",
    "# # - merged_long: long format with columns [country, year, IndicatorCode, value]\n",
    "# # - merged_wide: wide format with IndicatorCodes as columns\n",
    "# # - merged_wide_named: wide format with human-readable column names \n",
    "\n",
    "# merged_wide_named.head()\n",
    "\n",
    "#########################\"\"# merged_long.to_csv(\"GHO_blood_indicators_long.csv\", index=False)\n",
    "##########################\"\"# merged_wide.to_csv(\"GHO_blood_indicators_wide.csv\", index=False)\n",
    "# merged_wide_named.to_csv(\"GHO_blood_indicators_wide_named.csv\", index=False) # FINAL OUTPUT ; TELECHARGE QUE CE FICHIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e41400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BloodHyper_who = pd.read_csv(\"https://melissa-aftisse.emi.u-bordeaux.fr/\")\n",
    "# print(BloodHyper_who.shape)\n",
    "# BloodHyper_who.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ab356b",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">10‚ÄìWikipedia (pages publiques)</h3>\n",
    "\n",
    "- **Objective**: r√©cup√©rer les donn√©es li√©es au sang (groupes sanguins, blood safety) et les fusionner dans une table coh√©rente.\n",
    "- **Sources initiales**:\n",
    "  - Wikipedia ‚Äî Blood type distribution by country (ABO/Rh): https://en.wikipedia.org/wiki/Blood_type_distribution_by_country\n",
    "  - Wikipedia ‚Äî Blood donation (screening & availability context): https://en.wikipedia.org/wiki/Blood_donation\n",
    "- **Licensing/Attribution**: Data programmatically extracted from Wikipedia (CC BY-SA 4.0). Links provided above.\n",
    "- **R√©sultat**: une table fusionn√©e par pays avec colonnes (A+, A-, B+, B-, O+, O-, Rh,... et m√©triques donation/safety si disponibles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Constants & Helpers\n",
    "# ----------------------------\n",
    "BLOOD_GROUPS_URL = \"https://en.wikipedia.org/wiki/Blood_type_distribution_by_country\"\n",
    "DONATION_URL = \"https://en.wikipedia.org/wiki/Blood_donation\"  # best-effort exploration only\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/120.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Unicode minus used on Wikipedia sometimes\n",
    "UNICODE_MINUS = \"\\u2212\"\n",
    "\n",
    "# Standard column set we target\n",
    "CORE_COLS = [\"A+\", \"A-\", \"B+\", \"B-\", \"O+\", \"O-\", \"AB+\", \"AB-\"]\n",
    "PLUS_COLS = [\"A+\", \"B+\", \"O+\", \"AB+\"]\n",
    "MINUS_COLS = [\"A-\", \"B-\", \"O-\", \"AB-\"]\n",
    "\n",
    "# Map potential variations to standard\n",
    "COL_MAP = {\n",
    "    \"A+\": \"A+\", \"A-\": \"A-\", f\"A{UNICODE_MINUS}\": \"A-\",\n",
    "    \"B+\": \"B+\", \"B-\": \"B-\", f\"B{UNICODE_MINUS}\": \"B-\",\n",
    "    \"O+\": \"O+\", \"O-\": \"O-\", f\"O{UNICODE_MINUS}\": \"O-\",\n",
    "    \"AB+\": \"AB+\", \"AB-\": \"AB-\", f\"AB{UNICODE_MINUS}\": \"AB-\",\n",
    "}\n",
    "\n",
    "COUNTRY_COL_ALIASES = {\"Country\", \"Nation\", \"Country or region\", \"Sovereign state\"}\n",
    "BRACKET_FOOTNOTE = re.compile(r\"\\[[^\\]]*\\]\")\n",
    "PERCENT_RE = re.compile(r\"([0-9]+(?:\\.[0-9]+)?)\")\n",
    "\n",
    "\n",
    "def _strip_footnotes(text: str) -> str:\n",
    "    return BRACKET_FOOTNOTE.sub(\"\", str(text)).strip()\n",
    "\n",
    "\n",
    "def clean_country(val: str) -> str:\n",
    "    # Remove footnotes and parentheses content like \"(estimate)\"\n",
    "    v = _strip_footnotes(val)\n",
    "    v = re.sub(r\"\\([^\\)]*\\)\", \"\", v)  # remove parenthetical notes\n",
    "    return re.sub(r\"\\s+\", \" \", v).strip()\n",
    "\n",
    "\n",
    "def clean_percent_cell(val) -> Optional[float]:\n",
    "    if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "        return None\n",
    "    s = str(val).strip()\n",
    "    if s in {\"‚Äî\", \"‚Äì\", \"-\", \"NaN\", \"nan\", \"N/A\", \"\"}:\n",
    "        return None\n",
    "    s = _strip_footnotes(s)\n",
    "    s = s.replace(\"%\", \"\").strip()\n",
    "    m = PERCENT_RE.search(s)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(1))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def rename_blood_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Normalize column names by trimming and mapping unicode minus\n",
    "    new_cols = {}\n",
    "    for c in df.columns:\n",
    "        base = str(c).strip()\n",
    "        base = base.replace(UNICODE_MINUS, \"-\")\n",
    "        new_cols[c] = COL_MAP.get(base, base)\n",
    "    df = df.rename(columns=new_cols)\n",
    "    return df\n",
    "\n",
    "\n",
    "def pick_country_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for cand in list(df.columns):\n",
    "        base = str(cand).strip()\n",
    "        if base in COUNTRY_COL_ALIASES or base.lower() == \"country\":\n",
    "            return cand\n",
    "    # Sometimes the first column is country\n",
    "    first = df.columns[0]\n",
    "    return first if str(first).lower().startswith(\"country\") else None\n",
    "\n",
    "\n",
    "def normalize_blood_table(df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    df = rename_blood_columns(df.copy())\n",
    "    country_col = pick_country_column(df)\n",
    "    if country_col is None:\n",
    "        return None\n",
    "    df[country_col] = df[country_col].map(clean_country)\n",
    "\n",
    "    # Keep only country + any recognized blood columns\n",
    "    keep_cols = [country_col] + [c for c in CORE_COLS if c in df.columns]\n",
    "    df = df[keep_cols].copy()\n",
    "\n",
    "    # Coerce numeric percent columns\n",
    "    for col in CORE_COLS:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(clean_percent_cell)\n",
    "\n",
    "    # Drop rows without country name\n",
    "    df = df[df[country_col].astype(str).str.len() > 0]\n",
    "\n",
    "    # Rename country column to standard\n",
    "    df = df.rename(columns={country_col: \"Country\"})\n",
    "\n",
    "    # Add any missing blood columns as NaN\n",
    "    for col in CORE_COLS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Compute Others = 100 - sum(known) when possible\n",
    "    df[\"sum_known\"] = df[CORE_COLS].apply(lambda r: sum(v for v in r if isinstance(v, (int, float)) and not math.isnan(v)), axis=1)\n",
    "    df[\"Others\"] = df[\"sum_known\"].apply(lambda x: None if x == 0 else max(0.0, 100.0 - x))\n",
    "    df = df.drop(columns=[\"sum_known\"])  # tidy\n",
    "\n",
    "    # Deduplicate by country (take first non-null row if duplicates)\n",
    "    df = df.groupby(\"Country\", as_index=False).first()\n",
    "    return df[[\"Country\"] + CORE_COLS + [\"Others\"]]\n",
    "\n",
    "\n",
    "def fetch_tables(url: str) -> List[pd.DataFrame]:\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    tables = pd.read_html(resp.text, flavor=\"lxml\")\n",
    "    return tables\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Fetch blood type distribution\n",
    "# ----------------------------\n",
    "try:\n",
    "    tables_bg = fetch_tables(BLOOD_GROUPS_URL)\n",
    "    candidates: List[pd.DataFrame] = []\n",
    "    for t in tables_bg:\n",
    "        norm = normalize_blood_table(t)\n",
    "        if norm is not None:\n",
    "            # Heuristic: we expect at least half of CORE_COLS present as numbers\n",
    "            present = sum(col in norm.columns for col in CORE_COLS)\n",
    "            if present >= 6:\n",
    "                candidates.append(norm)\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"No suitable blood distribution table found on the page.\")\n",
    "\n",
    "    # Pick the widest table (most non-null values)\n",
    "    bg_df = max(candidates, key=lambda d: d[CORE_COLS].notna().sum().sum())\n",
    "    # Some countries may have percentages that don't sum to 100 due to rounding or missing values\n",
    "    missing_cols = [c for c in CORE_COLS if c not in bg_df.columns]\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to extract blood group distribution: {e}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Optional: Explore donation/safety metrics on Wikipedia\n",
    "# Note: Many Wikipedia pages don't have a structured per-country donation table.\n",
    "# We'll try to find a table with a country column and a donation metric.\n",
    "# If not found, we continue with blood distribution only.\n",
    "# ----------------------------\n",
    "\n",
    "def try_extract_donation_table(url: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        tables_dn = fetch_tables(url)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    for t in tables_dn:\n",
    "        t = t.copy()\n",
    "        t = rename_blood_columns(t)\n",
    "        ccol = pick_country_column(t)\n",
    "        if not ccol:\n",
    "            continue\n",
    "        # Look for any column that hints at donation rate/units\n",
    "        metric_cols = [c for c in t.columns if any(k in str(c).lower() for k in [\"donation\", \"per 1000\", \"units\", \"collections\", \"collected\"]) and c != ccol]\n",
    "        if not metric_cols:\n",
    "            continue\n",
    "        # Clean country\n",
    "        t[ccol] = t[ccol].map(clean_country)\n",
    "        # Keep first suitable metric column (best-effort)\n",
    "        mcol = metric_cols[0]\n",
    "        # Coerce metric to numeric if possible\n",
    "        t[mcol] = t[mcol].map(lambda v: (float(PERCENT_RE.search(str(v)).group(1)) if PERCENT_RE.search(str(v)) else None))\n",
    "        out = t[[ccol, mcol]].rename(columns={ccol: \"Country\", mcol: \"DonationMetric\"}).dropna(subset=[\"Country\"]).copy()\n",
    "        # Collapse duplicates\n",
    "        out = out.groupby(\"Country\", as_index=False).first()\n",
    "        return out\n",
    "    return None\n",
    "\n",
    "\n",
    "try:\n",
    "    don_df = try_extract_donation_table(DONATION_URL)\n",
    "except Exception:\n",
    "    don_df = None\n",
    "\n",
    "# ----------------------------\n",
    "# Merge & Save\n",
    "# ----------------------------\n",
    "merged = bg_df.copy()\n",
    "if don_df is not None:\n",
    "    merged = merged.merge(don_df, on=\"Country\", how=\"left\")\n",
    "\n",
    "# Ensure final column order\n",
    "preview_cols = [\"Country\"] + CORE_COLS + [\"Others\"] + ([\"DonationMetric\"] if \"DonationMetric\" in merged.columns else [])\n",
    "merged = merged[preview_cols]\n",
    "\n",
    "BloodTypes_wiki = merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on a copy of the original dataframe\n",
    "df = BloodTypes_wiki.copy()\n",
    "\n",
    "# Ensure expected columns exist\n",
    "req_plus = [\"A+\",\"B+\",\"O+\",\"AB+\"]\n",
    "req_minus = [\"A-\",\"B-\",\"O-\",\"AB-\"]\n",
    "missing = [c for c in req_plus + req_minus if c not in df.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing expected ABO columns: {missing}\")\n",
    "\n",
    "# Derive Rh(D) Pos / Neg\n",
    "df[\"Rh(D) Pos\"] = df[req_plus].sum(axis=1, skipna=True)\n",
    "df[\"Rh(D) Neg\"] = df[req_minus].sum(axis=1, skipna=True)\n",
    "\n",
    "# Optional sanity check column\n",
    "df[\"Rh_TotalKnown\"] = df[\"Rh(D) Pos\"] + df[\"Rh(D) Neg\"]\n",
    "\n",
    "# Overwrite or reassign back to the original variable if desired\n",
    "BloodTypes_wiki = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03885041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich BloodTypes_wiki with Population column from Wikipedia\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Ensure source URL exists\n",
    "try:\n",
    "    BLOOD_GROUPS_URL\n",
    "except NameError:\n",
    "    BLOOD_GROUPS_URL = \"https://en.wikipedia.org/wiki/Blood_type_distribution_by_country\"\n",
    "\n",
    "def _clean_country_name(s):\n",
    "    if pd.isna(s):\n",
    "        return s\n",
    "    # Remove footnote markers like [a], [1], etc., and bracketed notes\n",
    "    s = re.sub(r\"\\[[^\\]]*\\]\", \"\", str(s))\n",
    "    s = re.sub(r\"\\s*\\([^)]*\\)\\s*\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def _to_int_pop(v):\n",
    "    if pd.isna(v):\n",
    "        return pd.NA\n",
    "    s = str(v).replace(\",\", \"\").strip()\n",
    "    try:\n",
    "        return int(float(s))\n",
    "    except Exception:\n",
    "        return pd.NA\n",
    "\n",
    "def fetch_population_df(url: str) -> pd.DataFrame:\n",
    "    # Prefer already-parsed tables from earlier cells to avoid HTTP issues\n",
    "    tables = None\n",
    "    if \"tables_bg\" in globals() and isinstance(globals()[\"tables_bg\"], list) and len(globals()[\"tables_bg\"]) > 0:\n",
    "        tables = globals()[\"tables_bg\"]\n",
    "    else:\n",
    "        session = globals().get(\"SESSION\") or requests.Session()\n",
    "        headers = globals().get(\"HEADERS\", {\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        resp = session.get(url, headers=headers, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        tables = pd.read_html(resp.text, flavor=\"lxml\")\n",
    "    # Try to find a table containing Population and country names\n",
    "    candidates = []\n",
    "    for t in tables:\n",
    "        cols = [str(c).strip() for c in t.columns]\n",
    "        if any(re.search(r\"(?i)population\", str(c)) for c in cols) and any(re.search(r\"(?i)country|dependency\", str(c)) for c in cols):\n",
    "            candidates.append(t)\n",
    "    if not candidates:\n",
    "        # Fallback: the first table is typically the country distribution\n",
    "        candidates = [tables[0]]\n",
    "    dfp = candidates[0].copy()\n",
    "    # Standardize columns\n",
    "    rename_map = {}\n",
    "    for c in dfp.columns:\n",
    "        cs = str(c)\n",
    "        if re.search(r\"(?i)country|dependency\", cs):\n",
    "            rename_map[c] = \"Country/Dependency\"\n",
    "        elif re.search(r\"(?i)population\", cs):\n",
    "            rename_map[c] = \"Population\"\n",
    "    if rename_map:\n",
    "        dfp = dfp.rename(columns=rename_map)\n",
    "    # If multiindex or unexpected header rows, flatten\n",
    "    if isinstance(dfp.columns, pd.MultiIndex):\n",
    "        dfp.columns = [\" \".join([str(x) for x in lvl]).strip() for lvl in dfp.columns]\n",
    "    # Keep only required cols if present\n",
    "    keep = [c for c in [\"Country/Dependency\", \"Population\"] if c in dfp.columns]\n",
    "    dfp = dfp[keep] if keep else dfp\n",
    "    # Clean country names and population\n",
    "    if \"Country/Dependency\" in dfp.columns:\n",
    "        dfp[\"Country/Dependency\"] = dfp[\"Country/Dependency\"].map(_clean_country_name)\n",
    "    if \"Population\" in dfp.columns:\n",
    "        dfp[\"Population\"] = dfp[\"Population\"].map(_to_int_pop)\n",
    "    # Drop duplicates, keep first\n",
    "    if \"Country/Dependency\" in dfp.columns:\n",
    "        dfp = dfp.dropna(subset=[\"Country/Dependency\"]).drop_duplicates(subset=[\"Country/Dependency\"])\n",
    "    return dfp\n",
    "\n",
    "# Merge into BloodTypes_wiki\n",
    "try:\n",
    "    df_bt = BloodTypes_wiki.copy()\n",
    "except NameError:\n",
    "    raise RuntimeError(\"BloodTypes_wiki not found. Run the Wikipedia scraping cells first.\")\n",
    "\n",
    "pop_df = fetch_population_df(BLOOD_GROUPS_URL)\n",
    "if \"Country/Dependency\" in df_bt.columns:\n",
    "    df_bt[\"Country/Dependency\"] = df_bt[\"Country/Dependency\"].map(_clean_country_name)\n",
    "else:\n",
    "    # Try alternative country column names\n",
    "    for alt in [\"Country\", \"Country / Dependency\", \"Country or dependency\"]:\n",
    "        if alt in df_bt.columns:\n",
    "            df_bt = df_bt.rename(columns={alt: \"Country/Dependency\"})\n",
    "            df_bt[\"Country/Dependency\"] = df_bt[\"Country/Dependency\"].map(_clean_country_name)\n",
    "            break\n",
    "# Left join to preserve existing rows\n",
    "if \"Country/Dependency\" in df_bt.columns and \"Country/Dependency\" in pop_df.columns:\n",
    "    df_bt = df_bt.merge(pop_df, on=\"Country/Dependency\", how=\"left\")\n",
    "else:\n",
    "    # If join keys missing, append population as best-effort without merge\n",
    "    if \"Population\" in pop_df.columns and len(pop_df) == len(df_bt):\n",
    "        df_bt[\"Population\"] = pop_df[\"Population\"].values\n",
    "# Update global\n",
    "BloodTypes_wiki = df_bt\n",
    "display(BloodTypes_wiki.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09887d5",
   "metadata": {},
   "source": [
    "<h3 style=\"padding:10px; color:#FFF; background:#E3CFC6\">Jeux de donn√©es explor√©s mais non retenus</h3>\n",
    "\n",
    "<p>Au cours de notre recherche, plusieurs jeux de donn√©es ont √©t√© envisag√©s mais finalement √©cart√©s pour des raisons m√©thodologiques :</p>\n",
    "\n",
    "<strong> - <a href=\"https://www.kaggle.com/datasets/ninalabiba/blood-transfusion-dataset/data\" style=\"color :#882255; font-style: italic\">Blood Transfusion Dataset</a></strong>\n",
    "<ul>\n",
    "  <li><strong>Raison de l'exclusion</strong> : Seulement 4 variables explicatives (Recency, Frequency, Monetary, Time) avec 748 observations</li>\n",
    "  <li><strong>Limites identifi√©es</strong> : \n",
    "    <ul>\n",
    "      <li>Absence de donn√©es cliniques (pression art√©rielle, valeurs biologiques)</li>\n",
    "      <li>Pas d'informations d√©mographiques d√©taill√©es (√¢ge, sexe, groupe sanguin)</li>\n",
    "      <li>Donn√©es trop simplistes pour des mod√®les pr√©dictifs robustes</li>\n",
    "      <li>Impossibilit√© de croiser avec d'autres sources pour enrichir l'analyse</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<strong> - <a hearf= \"https://database.coffeeinstitute.org/\" style=\"color :#882255; font-style: italic\">Coffee Quality Institute Database</a></strong>\n",
    "<ul>\n",
    "  <li><strong>Raison du changement de th√©matique</strong> : Th√®me initial abandonn√© au profit du don de sang</li>\n",
    "  <li><strong>Limites constat√©es</strong> :\n",
    "    <ul>\n",
    "      <li>Manque de diversit√© des sources (d√©pendance √† une seule base principale)</li>\n",
    "      <li>Nombreux datasets d√©riv√©s ou redondants sur Kaggle</li>\n",
    "      <li>Donn√©es peu fiables pour certaines sources secondaires</li>\n",
    "      <li>Th√©matique moins align√©e avec nos int√©r√™ts en sciences de la sant√©</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<strong style=\"color :#882255; font-style: italic\">Tableau r√©capitulatif des datasets</strong>\n",
    "\n",
    "| # | Dataset | Source | Statut |\n",
    "|---|---------|--------|--------|\n",
    "| 1 | HyperRisk_kaggle | Kaggle (Hypertension Risk Model) | Utilis√© |\n",
    "| 2 | BloodPressure_kaggle | Kaggle (Blood Pressure) | Utilis√© |\n",
    "| 3 | BloodValues_kaggle | Kaggle (Blood Values) | Utilis√© |\n",
    "| 4 | BloodGroupWW_kaggle | Kaggle (distribution par pays) | TODO |\n",
    "| 5 | BloodDonation_kaggle | Kaggle (Donation Portal) | Utilis√© |\n",
    "| 6 | BloodIndia_gouv | data.gov.in | Utilis√© |\n",
    "| 7 | BCCD_github | GitHub (images + XML) | Utilis√© |\n",
    "| 8 | Mayo Clinic | Site public (HTML) | Non utilis√©  |\n",
    "| 9 | WHO (GHO API) | WHO | Prototype |\n",
    "| 10 | BloodTypes_wiki | Wikipedia | TODO |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9f6c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 style=\"padding:10px; color:#FFF; background:#882255\">PHASE 1 : EXPLORATION & NETTOYAGE</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b9183",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 1.1 : Qualit√© et Compl√©tude des Donn√©es</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 1 (`HyperRisk_kaggle`) + 2 (`BloodPressure_kaggle`) + 3 (`BloodValues_kaggle`) + 5 (`BloodDonation_kaggle`)\n",
    "\n",
    "**Visualisations :**\n",
    "- Heatmap des valeurs manquantes (seaborn)\n",
    "- Graphiques en barres des taux de compl√©tude par colonne\n",
    "- Boxplots pour d√©tecter les outliers par variable clinique\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Quelles sont les variables avec le plus de donn√©es manquantes ?\n",
    "- Y a-t-il des valeurs aberrantes dans les mesures cliniques (BP, glucose, cholest√©rol) ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d442e5",
   "metadata": {},
   "source": [
    "| √âtape de nettoyage | Dataset 1 | Dataset 2 | Dataset 3 | Dataset 5 |\n",
    "|-------------------|-----------|-----------|-----------|-----------|\n",
    "| Valeurs manquantes | Imputer par **m√©diane** les variables cliniques continues avec faible taux manquant : **BPMeds, totChol, cigsPerDay, BMI, heartRate** (<2%).<br>Pour **glucose (~9%)** : imputer par **m√©diane conditionnelle** ou cr√©er cat√©gorie **glucose_missing**.<br>Supprimer les lignes ayant **‚â• 3 variables cliniques critiques manquantes**. | Supprimer `Pregnancy` et `Patient_Number`.<br>Imputer les valeurs manquantes par la m√©diane pour : `Level_of_Hemoglobin`, `Genetic_Pedigree_Coefficient`, `alcohol_consumption_per_day`, `salt_content_in_the_diet`, `Age`, `BMI`, `Level_of_Stress`. |  | Aucune valeur manquante d√©tect√©e : aucune imputation n√©cessaire. |\n",
    "| Outliers | V√©rifier coh√©rence physiologique (**glucose** 50‚Äì400 mg/dL, **totChol** 100‚Äì400 mg/dL, **BMI** 15‚Äì60, **heartRate** 40‚Äì200 bpm, **cigsPerDay** 0‚Äì100).<br>Winsoriser aux **1er / 99e percentiles**.<br>Ajouter indicateur binaire **is_outlier** par variable ou global. | √âtudier et plafonner les valeurs aberrantes de `Level_of_Hemoglobin` (m√©thode IQR). |  | G√©rer les valeurs aberrantes : plafonnement IQR ou winsorisation pour `Age`, `Total_Donations`, `Weight_kg`, `Hemoglobin_g_dL`. |\n",
    "| Types / normalisation | Standardiser variables num√©riques : **glucose, totChol, BMI, heartRate, cigsPerDay**.<br>Binning clinique pertinent si n√©cessaire (ex. glucose : normal / pr√©diab√®te / diab√®te). | Normaliser variables asym√©triques, notamment `salt_content_in_the_diet` (log1p). |  | Valider plages biologiques et normaliser si n√©cessaire : `Age`, `Weight_kg`, `Hemoglobin_g_dL`. |\n",
    "| Encodage | V√©rifier et harmoniser variables binaires **(BPMeds, diabetes, smoker)** en `{0,1}`.<br>Encodage ordinal coh√©rent pour variables de fr√©quence (ex. cigsPerDay si discr√©tis√©e). | V√©rifier coh√©rence logique (ex: `Sex` vs `Pregnancy`). |  | Encoder variables cat√©gorielles : transformer `Blood_Group` et `Gender`. |\n",
    "| Doublons / coh√©rences | Supprimer doublons (`drop_duplicates()`).<br>V√©rifier r√®gles m√©tier simples (ex. cigsPerDay = 0 si non-fumeur).<br>Bloquer ou corriger valeurs impossibles avant mod√©lisation. | Valider plages de donn√©es (`Age` >0, `BMI` >0, `alcohol_consumption_per_day` ‚â•0). |  | Supprimer ou masquer identifiants : `Donor_ID`, `Full_Name`, `Email`, `Contact_Number`, `Last_Donation_Date`, `Donation_Center`, `Medical_Condition`. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a0f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(data_series):\n",
    "    \"\"\"D√©tecte les outliers dans une s√©rie de donn√©es en utilisant la m√©thode IQR.\"\"\"\n",
    "    Q1 = data_series.quantile(0.25)\n",
    "    Q3 = data_series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data_series[(data_series < lower_bound) | (data_series > upper_bound)]\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2543d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_quality(\n",
    "  df,\n",
    "  clinical_vars=None,\n",
    "  dataset_name=\"Dataset\",\n",
    "  completeness_threshold=95,\n",
    "  outlier_detector=None,\n",
    "  figsize=(16,12),\n",
    "  heatmap_cmap=\"YlOrRd\",\n",
    "  bar_color=\"#882255\",\n",
    "  outlier_bar_color=\"#E3CFC6\",\n",
    "  outlier_edge_color=\"#882255\",\n",
    "  show_plot=True,\n",
    "  return_results=True,\n",
    "):\n",
    "  print(\"=\"*70)\n",
    "  print(f\"{dataset_name} - Analyse de Qualit√©\")\n",
    "  print(\"=\"*70)\n",
    "\n",
    "  def transpose_quality_table(df,var_col=\"Colonne\"):\n",
    "    idx=df[var_col]\n",
    "    return df.drop(columns=[var_col]).T.set_axis(idx,axis=1)\n",
    "\n",
    "  missing_data_df=pd.DataFrame({\n",
    "    \"Nombre_manquants\":df.isnull().sum(),\n",
    "    \"Pourcentage\":(df.isnull().sum()/len(df)*100).round(2)\n",
    "  })\n",
    "  missing_data_df=missing_data_df[missing_data_df[\"Nombre_manquants\"]>0]\\\n",
    "    .sort_values(\"Nombre_manquants\",ascending=False)\n",
    "\n",
    "  print(\"\\n‚ùå Valeurs manquantes par colonne:\")\n",
    "  if missing_data_df.empty:\n",
    "    print(\"Aucune valeur manquante d√©tect√©e ‚úÖ\")\n",
    "  else:\n",
    "    missing_data_tbl=transpose_quality_table(\n",
    "      missing_data_df.reset_index().rename(columns={\"index\":\"Colonne\"})\n",
    "    )\n",
    "    display(\n",
    "      missing_data_tbl.style\n",
    "      .format(\"{:.2f}\")\n",
    "      .background_gradient(cmap=\"Reds\",axis=1)\n",
    "      .set_properties(**{\"text-align\":\"center\"})\n",
    "    )\n",
    "\n",
    "  fig,axes=plt.subplots(2,2,figsize=figsize)\n",
    "\n",
    "  sns.heatmap(df.isnull(),cbar=True,cmap=heatmap_cmap,\n",
    "              ax=axes[0,0],yticklabels=False)\n",
    "  axes[0,0].set_title(\n",
    "    f\"Heatmap des Valeurs Manquantes - {dataset_name}\",\n",
    "    fontsize=14,fontweight=\"bold\"\n",
    "  )\n",
    "\n",
    "  completeness=(1-df.isnull().sum()/len(df))*100\n",
    "  completeness_sorted=completeness.sort_values()\n",
    "  axes[0,1].barh(range(len(completeness_sorted)),\n",
    "                 completeness_sorted.values,color=bar_color)\n",
    "  axes[0,1].set_yticks(range(len(completeness_sorted)))\n",
    "  axes[0,1].set_yticklabels(completeness_sorted.index,fontsize=9)\n",
    "  axes[0,1].set_xlabel(\"Taux de Compl√©tude (%)\",fontsize=11)\n",
    "  axes[0,1].set_title(\n",
    "    f\"Taux de Compl√©tude par Colonne - {dataset_name}\",\n",
    "    fontsize=14,fontweight=\"bold\"\n",
    "  )\n",
    "  if completeness_threshold is not None:\n",
    "    axes[0,1].axvline(x=completeness_threshold,color=\"red\",\n",
    "                      linestyle=\"--\",\n",
    "                      label=f\"Seuil {completeness_threshold}%\")\n",
    "    axes[0,1].legend()\n",
    "\n",
    "  print(\"\\nüìà Taux de compl√©tude :\")\n",
    "  comp_df=transpose_quality_table(\n",
    "    completeness_sorted.rename(\"Taux (%)\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\":\"Colonne\"})\n",
    "  )\n",
    "  display(\n",
    "    comp_df.style\n",
    "    .format(\"{:.2f}%\")\n",
    "    .background_gradient(cmap=\"PuBu\",axis=1)\n",
    "    .set_properties(**{\"text-align\":\"center\"})\n",
    "  )\n",
    "\n",
    "  if clinical_vars is None:\n",
    "    clinical_vars=list(df.select_dtypes(include=\"number\").columns)\n",
    "  available_vars=[c for c in clinical_vars if c in df.columns]\n",
    "  if available_vars:\n",
    "    df[available_vars].boxplot(ax=axes[1,0])\n",
    "    axes[1,0].set_title(\n",
    "      f\"Boxplots - Variables Cliniques - {dataset_name}\",\n",
    "      fontsize=14,fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1,0].set_ylabel(\"Valeurs\",fontsize=11)\n",
    "    axes[1,0].tick_params(axis=\"x\",rotation=45,labelsize=8)\n",
    "  else:\n",
    "    axes[1,0].set_visible(False)\n",
    "\n",
    "  def _default_outlier_detector(s):\n",
    "    q1,q3=s.quantile(0.25),s.quantile(0.75)\n",
    "    iqr=q3-q1\n",
    "    return s[(s<q1-1.5*iqr)|(s>q3+1.5*iqr)]\n",
    "\n",
    "  detector=outlier_detector or _default_outlier_detector\n",
    "  outlier_counts={}\n",
    "  for col in available_vars:\n",
    "    s=df[col].dropna()\n",
    "    try: outliers=detector(s)\n",
    "    except Exception: outliers=_default_outlier_detector(s)\n",
    "    outlier_counts[col]=len(outliers)\n",
    "\n",
    "  if outlier_counts:\n",
    "    axes[1,1].bar(outlier_counts.keys(),\n",
    "                  outlier_counts.values(),\n",
    "                  color=outlier_bar_color,\n",
    "                  edgecolor=outlier_edge_color,\n",
    "                  linewidth=2)\n",
    "    axes[1,1].set_title(\n",
    "      f\"Nombre d'Outliers par Variable - {dataset_name}\",\n",
    "      fontsize=14,fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1,1].set_ylabel(\"Nombre d'outliers\",fontsize=11)\n",
    "    axes[1,1].tick_params(axis=\"x\",rotation=45,labelsize=8)\n",
    "\n",
    "    print(\"\\n‚ö†Ô∏è Outliers par variable :\")\n",
    "    outlier_df=transpose_quality_table(\n",
    "      pd.Series(outlier_counts)\n",
    "      .rename(\"Nombre d'outliers\")\n",
    "      .sort_values(ascending=False)\n",
    "      .reset_index()\n",
    "      .rename(columns={\"index\":\"Colonne\"})\n",
    "    )\n",
    "    display(\n",
    "      outlier_df.style\n",
    "      .format(\"{:,.0f}\")\n",
    "      .background_gradient(cmap=\"OrRd\",axis=1)\n",
    "      .set_properties(**{\"text-align\":\"center\"})\n",
    "    )\n",
    "  else:\n",
    "    axes[1,1].set_visible(False)\n",
    "    print(\"\\n‚ö†Ô∏è Outliers par variable: Aucun outlier d√©tect√© ‚úÖ\")\n",
    "\n",
    "  plt.tight_layout()\n",
    "  if show_plot: plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2543d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mad_outliers(s, k=3.5):\n",
    "    s = s.dropna()\n",
    "    if s.empty:\n",
    "        return s.iloc[0:0]\n",
    "    med = s.median()\n",
    "    mad = np.median(np.abs(s - med))\n",
    "    if mad == 0:\n",
    "        return s.iloc[0:0]\n",
    "    z = np.abs(s - med) / (1.4826 * mad)\n",
    "    return s[z > k]\n",
    "\n",
    "def mixed_outlier_detector(series):\n",
    "    name = str(getattr(series, 'name', '')).strip()\n",
    "    if name == 'salt_content_in_the_diet':\n",
    "        return _mad_outliers(series, k=3.5)\n",
    "    # Default IQR-based detection\n",
    "    q1, q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    return series[(series < lower) | (series > upper)]        \n",
    "\n",
    "def cap_iqr(s):\n",
    "  q1,q3=s.quantile(0.25),s.quantile(0.75);iqr=q3-q1\n",
    "  return s.clip(q1-1.5*iqr,q3+1.5*iqr)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a368760",
   "metadata": {},
   "source": [
    "<h4 style=\"padding:10px; color:#FFF; background:#E3CFC6\">1‚ÄìHyperRisk_kaggle</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a368760",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_vars = [\n",
    "  \"glucose\",\n",
    "  \"BPMeds\",\n",
    "  \"totChol\",\n",
    "  \"cigsPerDay\",\n",
    "  \"BMI\",\n",
    "  \"heartRate\",\n",
    "  \"currentSmoker\",\n",
    "  \"male\",\n",
    "  \"sysBP\",\n",
    "  \"diabetes\",\n",
    "  \"age\",\n",
    "  \"diaBP\"\n",
    "]\n",
    "\n",
    "analyze_dataset_quality(\n",
    "    HyperRisk_kaggle,\n",
    "    clinical_vars=clinical_vars,\n",
    "    dataset_name=\"DATASET 1: HyperRisk_kaggle\",\n",
    "    completeness_threshold=95\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a1eff",
   "metadata": {},
   "source": [
    "üßπ Strat√©gies de Nettoyage pour Dataset *HyperRisk_kaggle*\n",
    "\n",
    "* Valeurs manquantes\n",
    "- Imputer par **m√©diane** les variables cliniques continues avec faible taux manquant :  \n",
    "  **BPMeds, totChol, cigsPerDay, BMI, heartRate** (<2%).\n",
    "- Pour **glucose (~9%)** :\n",
    "  - imputer par **m√©diane conditionnelle** (par √¢ge ou statut diab√©tique si disponible), **ou**\n",
    "  - cr√©er une cat√©gorie **glucose_missing** si utilis√©e comme variable cl√©.\n",
    "- Supprimer les lignes ayant **‚â• 3 variables cliniques critiques manquantes**.\n",
    "\n",
    "* Outliers\n",
    "- V√©rifier la **coh√©rence physiologique** :\n",
    "  - **glucose** (50‚Äì400 mg/dL)\n",
    "  - **totChol** (100‚Äì400 mg/dL)\n",
    "  - **BMI** (15‚Äì60)\n",
    "  - **heartRate** (40‚Äì200 bpm)\n",
    "  - **cigsPerDay** (0‚Äì100)\n",
    "- Winsoriser aux **1er / 99e percentiles** pour limiter l‚Äôimpact des valeurs extr√™mes.\n",
    "- Ajouter un indicateur binaire **is_outlier** par variable ou global.\n",
    "\n",
    "* Types / normalisation\n",
    "- Standardiser les variables num√©riques :  \n",
    "  **glucose, totChol, BMI, heartRate, cigsPerDay**.\n",
    "- Binning clinique pertinent si n√©cessaire  \n",
    "  *(ex. glucose : normal / pr√©diab√®te / diab√®te)*.\n",
    "\n",
    "* Encodage\n",
    "- V√©rifier et harmoniser les variables binaires  \n",
    "  **(BPMeds, diabetes, smoker si pr√©sentes)** en `{0,1}`.\n",
    "- Encodage ordinal coh√©rent pour variables de fr√©quence  \n",
    "  *(cigsPerDay si discr√©tis√©e)*.\n",
    "\n",
    "* Doublons / coh√©rences\n",
    "- Supprimer les doublons (`drop_duplicates()`).\n",
    "- V√©rifier les r√®gles m√©tier simples  \n",
    "  *(ex. cigsPerDay = 0 si non-fumeur)*.\n",
    "- Bloquer ou corriger les valeurs impossibles avant mod√©lisation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Load dataset =================\n",
    "df = HyperRisk_kaggle.copy()\n",
    "\n",
    "# ================= Missing values =================\n",
    "num_vars = [\"glucose\", \"totChol\", \"BMI\", \"heartRate\", \"cigsPerDay\"]\n",
    "\n",
    "for col in num_vars:\n",
    "  if col in df.columns:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "if \"BPMeds\" in df.columns:\n",
    "  df[\"BPMeds\"] = df[\"BPMeds\"].fillna(0)\n",
    "\n",
    "df = df[df.isnull().sum(axis=1) < 3]\n",
    "\n",
    "# ================= Outliers (winsorization) =================\n",
    "def winsorize(series):\n",
    "  low = series.quantile(0.01)\n",
    "  high = series.quantile(0.99)\n",
    "  return series.clip(low, high)\n",
    "\n",
    "df[\"is_outlier\"] = 0\n",
    "for col in num_vars:\n",
    "  if col in df.columns:\n",
    "    before = df[col].copy()\n",
    "    df[col] = winsorize(df[col])\n",
    "    df.loc[before != df[col], \"is_outlier\"] = 1\n",
    "\n",
    "# ================= Physiological bounds =================\n",
    "bounds = {\n",
    "  \"glucose\": (50, 400),\n",
    "  \"totChol\": (100, 400),\n",
    "  \"BMI\": (15, 60),\n",
    "  \"heartRate\": (40, 200),\n",
    "  \"cigsPerDay\": (0, 100)\n",
    "}\n",
    "\n",
    "for col, (low, high) in bounds.items():\n",
    "  if col in df.columns:\n",
    "    df = df[(df[col] >= low) & (df[col] <= high)]\n",
    "\n",
    "# ================= Normalization =================\n",
    "scaler = StandardScaler()\n",
    "df[num_vars] = scaler.fit_transform(df[num_vars])\n",
    "\n",
    "# ================= Remove duplicates (key columns) =================\n",
    "dup_cols = [\"age\", \"sex\", \"glucose\", \"BMI\"]\n",
    "dup_cols = [c for c in dup_cols if c in df.columns]\n",
    "\n",
    "df = df.drop_duplicates(subset=dup_cols, keep=\"first\")\n",
    "\n",
    "HyperRisk_kaggle = df\n",
    "\n",
    "print(\"Final shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "#-9 lignes apres suppression des duppliquants \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a35cd8",
   "metadata": {},
   "source": [
    "<h4 style=\"padding:10px; color:#FFF; background:#E3CFC6\">2‚ÄìBloodPressure_kaggle</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e499250",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 0.95em; margin-top: 8px;\">\n",
    "<strong>Note:</strong> Les variables √† faible cardinalit√© (p. ex. <em>Physical_activity</em>) sont trait√©es comme cat√©gorielles et exclues de l‚Äôanalyse IQR des outliers.\n",
    " Cela √©vite des faux positifs li√©s aux r√®gles des moustaches des boxplots sur des valeurs discr√®tes. L‚Äôanalyse des outliers utilise les colonnes num√©riques\n",
    " ayant plus de 10 modalit√©s distinctes.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e499250",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_vars = [c for c in BloodPressure_kaggle.select_dtypes(include=['number']).columns if BloodPressure_kaggle[c].nunique() > 10 and c != \"Physical_activity\" and c != \"salt_content_in_the_diet\" and c != \"Patient_Number\"]\n",
    "\n",
    "analyze_dataset_quality(\n",
    "    BloodPressure_kaggle,\n",
    "    clinical_vars=clinical_vars, \n",
    "    # clinical_vars=None,\n",
    "    dataset_name=\"DATASET 2: BloodPressure_kaggle\",\n",
    "    completeness_threshold=95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = {\"Sex\", \"Pregnancy\"}\n",
    "missing = [c for c in required_cols if c not in BloodPressure_kaggle.columns]\n",
    "if missing:\n",
    "    print(f\"Colonnes manquantes: {missing}\")\n",
    "else:\n",
    "    before_inconsistent = int(((BloodPressure_kaggle[\"Sex\"] == 0) & (BloodPressure_kaggle[\"Pregnancy\"] != 0)).sum())\n",
    "\n",
    "    BloodPressure_kaggle.loc[BloodPressure_kaggle[\"Sex\"] == 0, \"Pregnancy\"] = 0\n",
    "\n",
    "    after_inconsistent = int(((BloodPressure_kaggle[\"Sex\"] == 0) & (BloodPressure_kaggle[\"Pregnancy\"] != 0)).sum())\n",
    "\n",
    "    print(f\"Incoh√©rences (Sex=0 & Pregnancy>0) avant correction: {before_inconsistent}\")\n",
    "    print(f\"Incoh√©rences (Sex=0 & Pregnancy>0) apr√®s correction:  {after_inconsistent}\")\n",
    "\n",
    "\n",
    "print(f\"Valeurs manquantes = {BloodPressure_kaggle['Pregnancy'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fa97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=BloodPressure_kaggle.copy()\n",
    "\n",
    "df.drop(columns=[\"Pregnancy\",\"Patient_Number\"],errors=\"ignore\",inplace=True)\n",
    "\n",
    "num_cols=[\"Level_of_Hemoglobin\",\"Genetic_Pedigree_Coefficient\",\n",
    "          \"alcohol_consumption_per_day\",\"salt_content_in_the_diet\",\n",
    "          \"Age\",\"BMI\",\"Level_of_Stress\"]\n",
    "\n",
    "df[num_cols]=df[num_cols].apply(lambda c:c.fillna(c.median()))\n",
    "\n",
    "df[\"Level_of_Hemoglobin\"]=cap_iqr(df[\"Level_of_Hemoglobin\"])\n",
    "\n",
    "df[\"salt_content_in_the_diet\"]=np.log1p(df[\"salt_content_in_the_diet\"])\n",
    "\n",
    "df=df[(df[\"Age\"]>0)&(df[\"BMI\"]>0)&(df[\"alcohol_consumption_per_day\"]>=0)]\n",
    "\n",
    "BloodPressure_kaggle = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset_quality(\n",
    "    df,\n",
    "    clinical_vars=[c for c in df.select_dtypes(include=['number']).columns if df[c].nunique() > 10],\n",
    "    dataset_name=\"DATASET 2: BloodPressure_kaggle\",\n",
    "    completeness_threshold=95,\n",
    "    outlier_detector=mixed_outlier_detector\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee25cb",
   "metadata": {},
   "source": [
    "<h4 style=\"padding:10px; color:#FFF; background:#E3CFC6\">3‚ÄìBloodValues_kaggle</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967cb9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset_quality(\n",
    "    BloodValues_kaggle,\n",
    "    clinical_vars=None,\n",
    "    dataset_name=\"DATASET 3: BloodValues_kaggle\",\n",
    "    completeness_threshold=95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da540435",
   "metadata": {},
   "source": [
    "<h4 style=\"padding:10px; color:#FFF; background:#E3CFC6\">5‚ÄìBloodDonation_kaggle</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset_quality(\n",
    "    BloodDonation_kaggle,\n",
    "    clinical_vars=None,\n",
    "    dataset_name=\"DATASET 5: BloodDonation_kaggle\",\n",
    "    completeness_threshold=95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebeae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BloodDonation_kaggle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebeae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=BloodDonation_kaggle\n",
    "\n",
    "print(\"Initial shape:\",df.shape)\n",
    "\n",
    "df.drop(columns=[\"Donor_ID\",\"Full_Name\",\"Email\",\"Contact_Number\",\"Last_Donation_Date\",\n",
    "                 \"Donation_Center\",\"Medical_Condition\"],inplace=True)\n",
    "\n",
    "print(\"After dropping unused columns:\",df.shape)\n",
    "\n",
    "if (sum(BloodDonation_kaggle['Country'] != 'India') == 0):\n",
    "    df.drop(columns=[\"Country\"],inplace=True)\n",
    "\n",
    "print(\"After dropping Country column if only India:\",df.shape)\n",
    "\n",
    "df[\"Gender\"]=df[\"Gender\"].map({\"Male\":1,\"Female\":0})\n",
    "\n",
    "print(\"After mapping Gender:\",df.shape)\n",
    "\n",
    "df[\"Eligible_for_Donation\"]=df[\"Eligible_for_Donation\"].map({\"Yes\":1,\"No\":0})\n",
    "\n",
    "print(\"After mapping Eligible_for_Donation:\",df.shape)\n",
    "\n",
    "df[\"Registration_Date\"]=pd.to_datetime(df[\"Registration_Date\"],dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "df=df[(df[\"Age\"].between(18,65))&(df[\"Weight_kg\"].between(40,150))]\n",
    "\n",
    "print(\"After filtering Age and Weight_kg:\",df.shape)\n",
    "\n",
    "df=df[df[\"Hemoglobin_g_dL\"].between(10,20)]\n",
    "\n",
    "print(\"After filtering Hemoglobin_g_dL:\",df.shape)\n",
    "\n",
    "for c in [\"Age\",\"Total_Donations\",\"Weight_kg\",\"Hemoglobin_g_dL\"]:\n",
    "  df[c]=cap_iqr(df[c])\n",
    "\n",
    "print(\"After capping IQR on numeric columns:\",df.shape)  \n",
    "\n",
    "df=pd.get_dummies(df,columns=[\"Blood_Group\"],\n",
    "                   drop_first=True) # True = 1\n",
    "\n",
    "print(\"After one-hot encoding Blood_Group:\",df.shape)\n",
    "\n",
    "# Validate Gender column (should only contain 0 or 1)\n",
    "invalid_gender = BloodDonation_kaggle[~BloodDonation_kaggle['Gender'].isin([0, 1])]\n",
    "\n",
    "print(f\"Invalid Gender entries: {len(invalid_gender)}\")\n",
    "\n",
    "if len(invalid_gender) > 0:\n",
    "  df = df[df['Gender'].isin([0, 1])]\n",
    "\n",
    "print(\"After filtering invalid Gender entries:\",df.shape)\n",
    "\n",
    "BloodDonation_kaggle = df\n",
    "print(BloodDonation_kaggle.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb78029",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataset_quality(\n",
    "    BloodDonation_kaggle,\n",
    "    clinical_vars=None,\n",
    "    dataset_name=\"DATASET 5: BloodDonation_kaggle\",\n",
    "    completeness_threshold=95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11c30e",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 1.2 : üßπ Strat√©gies de Nettoyage par dataset</h3>\n",
    "\n",
    "| Strat√©gie | HyperRisk_kaggle | BloodPressure_kaggle | BloodValues_kaggle | BloodDonation_kaggle |\n",
    "|---|---|---|---|---|\n",
    "| Valeurs manquantes (num√©riques) | ‚Äî | ‚úÖ `Pregnancy`, `alcohol_consumption_per_day`, `Genetic_Pedigree_Coefficient` | ‚Äî | ‚ùå Aucune imputation explicite (filtrage physiologique appliqu√©) |\n",
    "| Imputer par m√©diane | ‚Äî | ‚úÖ `Level_of_Hemoglobin`, `Genetic_Pedigree_Coefficient`, `alcohol_consumption_per_day`, `salt_content_in_the_diet`, `Age`, `BMI`, `Level_of_Stress` | ‚Äî | ‚ùå Non |\n",
    "| Supprimer lignes avec ‚â• 3 variables cliniques manquantes | ‚Äî | ‚ùå Non | ‚Äî | ‚ùå Non |\n",
    "| Outliers | ‚Äî | D√©tect√©s ; correction appliqu√©e sur Level_of_Hemoglobin (IQR capping) | ‚Äî | ‚úÖ IQR / winsorisation (`Age`, `Total_Donations`, `Weight_kg`, `Hemoglobin_g_dL`) |\n",
    "| Coh√©rence physiologique (bornes) | ‚Äî | ‚úÖ R√®gle logique `Sex=0 ‚áí Pregnancy=0` + bornes physiologiques | ‚Äî | ‚úÖ Filtres stricts (`Age` 18‚Äì65, `Weight_kg` 40‚Äì150, `Hemoglobin_g_dL` 10‚Äì20) |\n",
    "| Winsoriser | ‚Äî | ‚úÖ Level_of_Hemoglobin (via `cap_iqr`) | ‚Äî | ‚úÖ Oui (via `cap_iqr`) |\n",
    "| Types/normalisation | ‚Äî | ‚úÖ log1p(salt_content_in_the_diet) (r√©duction de l‚Äôasym√©trie) | ‚Äî | ‚ùå Non (variables num√©riques conserv√©es telles quelles) |\n",
    "| Standardisation | ‚Äî | ‚ùå Non | ‚Äî | ‚Äî |\n",
    "| Encodage (cat√©gorielles) | ‚Äî | ‚ùå Non | ‚Äî | ‚úÖ Gender, Eligible_for_Donation binaires + one-hot encoding de Blood_Group |\n",
    "| Doublons | ‚Äî | ‚ùå Non d√©tect√©s | ‚Äî | ‚ùå Non d√©tect√©s |\n",
    "| Colonnes supprim√©es | ‚Äî | ‚úÖ `Pregnancy`, `Patient_Number` | ‚Äî | ‚úÖ `Donor_ID`, `Full_Name`, `Email`, `Contact_Number`, `Last_Donation_Date`, `Donation_Center`, `Medical_Condition`, `Country` |\n",
    "| Dates normalis√©es | ‚Äî | ‚ùå Non | ‚Äî | ‚úÖ `Registration_Date` convertie en datetime |\n",
    "| NB | ‚Äî | Imputation m√©diane + correction physiologique ; capping IQR sur `Level_of_Hemoglobin` ; transformation log sur `salt_content_in_the_diet`| ‚Äî | Nettoyage repose surtout sur **filtrage physiologique + capping IQR + encodage**, pas sur l‚Äôimputation |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3939b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 style=\"padding:10px; color:#FFF; background:#882255\">PHASE 2 : ANALYSES DESCRIPTIVES</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbd698",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 2.1 : Profils D√©mographiques des Donneurs</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 5 (`BloodDonation_kaggle`)\n",
    "\n",
    "**Visualisations :**\n",
    "- Pyramide des √¢ges (histogramme group√© par sexe)\n",
    "- Graphiques en secteurs pour la distribution des groupes sanguins\n",
    "- Barplots des conditions m√©dicales pr√©existantes\n",
    "- Distributions des intervalles entre dons (violin plots)\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Quelle est la r√©partition √¢ge/sexe des donneurs ?\n",
    "- Quelles sont les conditions m√©dicales les plus fr√©quentes ?\n",
    "- Quel est le profil type du donneur r√©gulier vs occasionnel ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af779db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Adjust one of these if your file is local\n",
    "# URL = \"https://melissa-aftisse.emi.u-bordeaux.fr/DATA/blood_donation.csv\"\n",
    "# LOCAL_CANDIDATES = [\"blooddonation_kaggle.csv\", \"blood_donation.csv\", \"blood_donation_kaggle.csv\"]\n",
    "\n",
    "# csv_path = next((p for p in LOCAL_CANDIDATES if pd.io.common.file_exists(p)), None)\n",
    "# if csv_path:\n",
    "#     df = pd.read_csv(csv_path)\n",
    "# else:\n",
    "#     df = pd.read_csv(URL)\n",
    "\n",
    "\n",
    "#df=BloodDonation_kaggle \n",
    "\n",
    "print(\"Data shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# Normalize column names (strip spaces, lowercase)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Heuristic column detection\n",
    "def find_col(df, keywords):\n",
    "    lc = {c: c.lower() for c in df.columns}\n",
    "    for k in keywords:\n",
    "        for c, clc in lc.items():\n",
    "            if k in clc:\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "age_col = find_col(df, [\"age\"])\n",
    "sex_col = find_col(df, [\"sex\", \"gender\", \"sexe\"])\n",
    "bg_col = find_col(df, [\"blood group\", \"blood_group\", \"bloodgroup\", \"blood type\", \"blood_type\", \"bloodtype\"]) or find_col(df, [\"blood\"])\n",
    "cond_col = find_col(df, [\"preexist\", \"condition\", \"medical\"])\n",
    "int_col = find_col(df, [\"interval\", \"interval_days\", \"days_between\", \"days\", \"days_since\"])\n",
    "\n",
    "# 1) Pyramide des √¢ges (histogramme group√© par sexe)\n",
    "if age_col and sex_col:\n",
    "    df_age = df.dropna(subset=[age_col, sex_col]).copy()\n",
    "    df_age[age_col] = pd.to_numeric(df_age[age_col], errors=\"coerce\")\n",
    "    bins = list(range(int(df_age[age_col].min())//5*5, int(df_age[age_col].max())//5*5 + 6, 5))\n",
    "    df_age['age_bin'] = pd.cut(df_age[age_col], bins=bins, right=False)\n",
    "    counts = df_age.groupby(['age_bin', sex_col]).size().unstack(fill_value=0)\n",
    "    sexes = [s for s in [\"Male\", \"Female\"] if s in counts.columns] + [c for c in counts.columns if c not in (\"Male\",\"Female\")]\n",
    "    if len(sexes) < 2:\n",
    "        sexes = counts.columns.tolist()\n",
    "    left = -counts[sexes[0]]\n",
    "    right = counts[sexes[1]] if len(sexes) > 1 else counts.iloc[:,0]\n",
    "    y = np.arange(len(counts.index))\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.barh(y, left, color='steelblue', alpha=0.85, label=sexes[0])\n",
    "    ax.barh(y, right, color='salmon', alpha=0.85, label=sexes[1] if len(sexes)>1 else 'Other')\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels([str(i) for i in counts.index])\n",
    "    ax.set_xlabel(\"Nombre (m√¢les n√©gatif pour pyramide)\")\n",
    "    ax.set_title(\"Pyramide des √¢ges par sexe\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Impossible de tracer la pyramide des √¢ges ‚Äî colonnes `age` ou `sex` introuvables.\")\n",
    "\n",
    "# 2) Graphiques en secteurs pour la distribution des groupes sanguins\n",
    "if bg_col:\n",
    "    bg_counts = df[bg_col].fillna(\"Unknown\").value_counts()\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.pie(bg_counts.values, labels=bg_counts.index, autopct=\"%1.1f%%\", startangle=90)\n",
    "    ax.set_title(\"Distribution des groupes sanguins\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Colonne groupe sanguin introuvable ‚Äî impossible de tracer les secteurs.\")\n",
    "\n",
    "# 3) Barplots des conditions m√©dicales pr√©existantes\n",
    "if cond_col:\n",
    "    if df[cond_col].dtype == object:\n",
    "        exploded = df[cond_col].fillna(\"None\").astype(str).str.split(\";\").explode().str.strip()\n",
    "        cond_counts = exploded.value_counts()\n",
    "    else:\n",
    "        cond_counts = df[cond_col].value_counts()\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    sns.barplot(x=cond_counts.index, y=cond_counts.values, palette=\"viridis\", ax=ax)\n",
    "    ax.set_ylabel(\"Nombre\")\n",
    "    ax.set_xlabel(\"Condition m√©dicale\")\n",
    "    ax.set_title(\"Conditions m√©dicales pr√©existantes\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Colonne conditions m√©dicales introuvable ‚Äî impossible de tracer le barplot.\")\n",
    "\n",
    "# 4) Violin plots : distribution des intervalles entre dons\n",
    "if int_col and int_col in df.columns:\n",
    "    df[int_col] = pd.to_numeric(df[int_col], errors=\"coerce\")\n",
    "    if sex_col and sex_col in df.columns:\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        sns.violinplot(x=sex_col, y=int_col, data=df, inner=\"quartile\", palette=\"Set2\", ax=ax)\n",
    "        ax.set_title(\"Distribution des intervalles entre dons par sexe\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(6,5))\n",
    "        sns.violinplot(y=int_col, data=df, inner=\"quartile\", color=\"lightblue\", ax=ax)\n",
    "        ax.set_title(\"Distribution des intervalles entre dons\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    \n",
    "    print(\"Colonne d'intervalle introuvable ‚Äî impossible de tracer les violin plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d64fe",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 2.2 : Distribution de la Pression Art√©rielle</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 2 (`BloodPressure_kaggle`)\n",
    "\n",
    "**Visualisations :**\n",
    "- Histogrammes de distribution (systolique et diastolique)\n",
    "- Scatter plot systolique vs diastolique avec zones de classification (normale/√©lev√©e/hypertension)\n",
    "- KDE plots pour comparer les distributions par sexe et par tranche d'√¢ge\n",
    "- Boxplots par cat√©gories d√©mographiques\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Quelle est la distribution normale de la pression art√©rielle dans la population ?\n",
    "- Comment d√©finir les seuils de risque (hypotension, pr√©hypertension, hypertension) ?\n",
    "- Y a-t-il des diff√©rences significatives entre hommes et femmes ?\n",
    "\n",
    "\n",
    "Dans cette section, nous analysons la distribution de la pression art√©rielle systolique (SBP) et diastolique (DBP), visualisons la relation SBP vs DBP avec zones cliniques (normale, √©lev√©e, hypertension), et comparons les distributions par sexe et par tranches d'√¢ge. Nous terminons par des boxplots pour explorer les diff√©rences d√©mographiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df09494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recherche de colonnes SBP/DBP dans d'autres datasets pr√©sents\n",
    "def print_cols(name):\n",
    "    try:\n",
    "        df = globals()[name]\n",
    "        cols = list(df.columns)\n",
    "        print(f\"\\n{name} colonnes ({len(cols)}):\", cols[:20])\n",
    "        def norm(s):\n",
    "            import re\n",
    "            return re.sub(r\"[^a-z]\", \"\", str(s).lower())\n",
    "        filtered = [c for c in cols if any(t in norm(c) for t in [\"bp\",\"press\",\"syst\",\"sys\",\"diast\",\"dia\",\"dbp\",\"sbp\"]) ]\n",
    "        print(f\"{name} colonnes SBP/DBP candidates:\", filtered)\n",
    "    except Exception as e:\n",
    "        print(f\"{name}:\", e)\n",
    "\n",
    "print_cols('BloodValues_kaggle')\n",
    "print_cols('HyperRisk_kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cf879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic rapide des colonnes du DataFrame BloodPressure_kaggle (filtr√©)\n",
    "try:\n",
    "    cols = list(BloodPressure_kaggle.columns)\n",
    "    print(\"Total colonnes:\", len(cols))\n",
    "    print(\"Colonnes (aper√ßu):\", cols[:20])\n",
    "    def norm(s):\n",
    "        import re\n",
    "        return re.sub(r\"[^a-z]\", \"\", str(s).lower())\n",
    "    filtered = [c for c in cols if any(t in norm(c) for t in [\"bp\",\"press\",\"syst\",\"sys\",\"diast\",\"dia\",\"dbp\",\"sbp\"])]\n",
    "    print(\"Colonnes potentiellement SBP/DBP:\", filtered)\n",
    "    print(\"\\nAper√ßu (5 lignes, colonnes filtr√©es):\")\n",
    "    preview_cols = filtered[:6] if filtered else cols[:6]\n",
    "    print(BloodPressure_kaggle[preview_cols].head().to_string())\n",
    "except NameError:\n",
    "    print(\"Variable 'BloodPressure_kaggle' non d√©finie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce06d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©paration des donn√©es : mapping colonnes + classification clinique\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Essayer d'utiliser BloodPressure_kaggle; sinon basculer sur HyperRisk_kaggle (contient sysBP/diaBP)\n",
    "source_name = None\n",
    "try:\n",
    "    df_bp = BloodPressure_kaggle.copy()\n",
    "    source_name = 'BloodPressure_kaggle'\n",
    "except NameError:\n",
    "    df_bp = None\n",
    "\n",
    "# Fonction utilitaires de mapping\n",
    "def _normalize(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z]\", \"\", str(s).lower())\n",
    "\n",
    "def _find_col(df: pd.DataFrame, aliases):\n",
    "    cols = list(df.columns)\n",
    "    norm = {c: _normalize(c) for c in cols}\n",
    "    # correspondance exacte\n",
    "    for alias in aliases:\n",
    "        key = _normalize(alias)\n",
    "        for c, n in norm.items():\n",
    "            if n == key:\n",
    "                return c\n",
    "    # correspondance partielle\n",
    "    for alias in aliases:\n",
    "        key = _normalize(alias)\n",
    "        for c, n in norm.items():\n",
    "            if key in n:\n",
    "                return c\n",
    "    # heuristiques larges (sys/syst, dia/diast)\n",
    "    for c, n in norm.items():\n",
    "        if any(t in n for t in [\"systolic\",\"sbp\",\"sys\"]):\n",
    "            return c\n",
    "    for c, n in norm.items():\n",
    "        if any(t in n for t in [\"diastolic\",\"dbp\",\"dia\"]):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "syst_alias = ['Systolic','Sys','SBP','SystolicBP','Systolic_Bp','Systolic(mmHg)']\n",
    "dias_alias = ['Diastolic','Dia','DBP','DiastolicBP','Diastolic_Bp','Diastolic(mmHg)']\n",
    "age_alias  = ['Age','Ages','AgeYears','age_years']\n",
    "sex_alias  = ['Sex','Gender','sex','gender']\n",
    "\n",
    "# Tenter sur BloodPressure_kaggle\n",
    "if df_bp is not None:\n",
    "    sys_col = _find_col(df_bp, syst_alias)\n",
    "    dia_col = _find_col(df_bp, dias_alias)\n",
    "    age_col = _find_col(df_bp, age_alias)\n",
    "    sex_col = _find_col(df_bp, sex_alias)\n",
    "else:\n",
    "    sys_col = dia_col = age_col = sex_col = None\n",
    "\n",
    "# Si pas de SBP/DBP, basculer sur HyperRisk_kaggle\n",
    "if sys_col is None or dia_col is None:\n",
    "    try:\n",
    "        df_bp = HyperRisk_kaggle.copy()\n",
    "        source_name = 'HyperRisk_kaggle'\n",
    "        # Colonnes explicites dans ce dataset\n",
    "        sys_col = 'sysBP'\n",
    "        dia_col = 'diaBP'\n",
    "        age_col = 'age' if 'age' in df_bp.columns else None\n",
    "        sex_col = 'male' if 'male' in df_bp.columns else None\n",
    "    except NameError:\n",
    "        raise RuntimeError(\"Aucune source avec SBP/DBP disponible (ni BloodPressure_kaggle ni HyperRisk_kaggle).\")\n",
    "\n",
    "# Standardiser les colonnes\n",
    "df_bp['Systolic']  = pd.to_numeric(df_bp[sys_col], errors='coerce')\n",
    "df_bp['Diastolic'] = pd.to_numeric(df_bp[dia_col], errors='coerce')\n",
    "if age_col:\n",
    "    df_bp['Age'] = pd.to_numeric(df_bp[age_col], errors='coerce')\n",
    "if sex_col:\n",
    "    if source_name == 'HyperRisk_kaggle':\n",
    "        df_bp['Sex'] = df_bp[sex_col].map({1:'Male', 0:'Female'}).astype(str)\n",
    "    else:\n",
    "        df_bp['Sex'] = df_bp[sex_col].astype(str).str.title().replace({'M':'Male','F':'Female'})\n",
    "\n",
    "df_bp = df_bp.dropna(subset=['Systolic','Diastolic']).copy()\n",
    "\n",
    "# Tranches d'√¢ge\n",
    "if 'Age' in df_bp.columns:\n",
    "    bins   = [18,30,40,50,60,70,200]\n",
    "    labels = ['18-29','30-39','40-49','50-59','60-69','70+']\n",
    "    df_bp['AgeGroup'] = pd.cut(df_bp['Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Classification clinique (AHA)\n",
    "def classify_bp(sys, dia):\n",
    "    if sys < 90 or dia < 60:\n",
    "        return 'Hypotension'\n",
    "    if sys < 120 and dia < 80:\n",
    "        return 'Normal'\n",
    "    if 120 <= sys <= 129 and dia < 80:\n",
    "        return 'Elevated'\n",
    "    if (130 <= sys <= 139) or (80 <= dia <= 89):\n",
    "        return 'Hypertension Stage 1'\n",
    "    if sys >= 140 or dia >= 90:\n",
    "        return 'Hypertension Stage 2'\n",
    "    return 'Unclassified'\n",
    "\n",
    "df_bp['BP_Category'] = np.vectorize(classify_bp)(df_bp['Systolic'], df_bp['Diastolic'])\n",
    "\n",
    "print(f\"Source utilis√©e: {source_name}\")\n",
    "# R√©sum√©s rapides\n",
    "counts = df_bp['BP_Category'].value_counts(normalize=True).mul(100).round(1)\n",
    "print(\"R√©partition des cat√©gories (%)\")\n",
    "print(counts.to_string())\n",
    "print(\"\\nStats globales (mean¬±std):\")\n",
    "print(f\"Systolic: {df_bp['Systolic'].mean():.1f} ¬± {df_bp['Systolic'].std():.1f}\")\n",
    "print(f\"Diastolic: {df_bp['Diastolic'].mean():.1f} ¬± {df_bp['Diastolic'].std():.1f}\")\n",
    "if 'Sex' in df_bp.columns:\n",
    "    gmean = df_bp.groupby('Sex')[['Systolic','Diastolic']].mean().round(1)\n",
    "    print(\"\\nMoyennes par sexe:\\n\", gmean.to_string())\n",
    "if 'AgeGroup' in df_bp.columns:\n",
    "    ag = df_bp.groupby('AgeGroup')[['Systolic','Diastolic']].mean().round(1)\n",
    "    print(\"\\nMoyennes par tranche d'√¢ge:\\n\", ag.to_string())\n",
    "\n",
    "# Conserver le DataFrame pr√©par√© pour les cellules suivantes\n",
    "prepared_bp = df_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# --- Sliders ---\n",
    "sys_hypo_w       = IntSlider(value=90,  min=60,  max=110, step=1, description='SBP hypo <')\n",
    "sys_norm_w       = IntSlider(value=120, min=100, max=140, step=1, description='SBP normal <')\n",
    "sys_elev_hi_w    = IntSlider(value=129, min=120, max=139, step=1, description='SBP √©lev√© ‚â§')\n",
    "sys_stage2_lo_w  = IntSlider(value=140, min=130, max=200, step=1, description='SBP HTN2 ‚â•')\n",
    "\n",
    "dia_hypo_w       = IntSlider(value=60,  min=40,  max=80,  step=1, description='DBP hypo <')\n",
    "dia_norm_w       = IntSlider(value=80,  min=60,  max=100, step=1, description='DBP normal <')\n",
    "dia_stage1_hi_w  = IntSlider(value=89,  min=80,  max=99,  step=1, description='DBP HTN1 ‚â§')\n",
    "dia_stage2_lo_w  = IntSlider(value=90,  min=85,  max=120, step=1, description='DBP HTN2 ‚â•')\n",
    "\n",
    "# --- Recompute function ---\n",
    "def recompute(sys_hypo, sys_norm, sys_elev_hi, sys_stage2_lo,\n",
    "              dia_hypo, dia_norm, dia_stage1_hi, dia_stage2_lo,\n",
    "              show_plots=True):\n",
    "    df = prepared_bp.copy()\n",
    "\n",
    "    # Classify function\n",
    "    def classify(sys, dia):\n",
    "        if sys < sys_hypo or dia < dia_hypo:\n",
    "            return 'Hypotension'\n",
    "        if sys < sys_norm and dia < dia_norm:\n",
    "            return 'Normal'\n",
    "        if (sys_norm <= sys <= sys_elev_hi) and (dia < dia_norm):\n",
    "            return 'Elevated'\n",
    "        if (sys_elev_hi < sys < sys_stage2_lo) or (dia_norm <= dia <= dia_stage1_hi):\n",
    "            return 'Hypertension Stage 1'\n",
    "        if (sys >= sys_stage2_lo) or (dia >= dia_stage2_lo):\n",
    "            return 'Hypertension Stage 2'\n",
    "        return 'Unclassified'\n",
    "\n",
    "    df['BP_Category'] = np.vectorize(classify)(df['Systolic'], df['Diastolic'])\n",
    "    counts = df['BP_Category'].value_counts(normalize=True).mul(100).round(1)\n",
    "    display(counts.to_frame('Pourcentage'))\n",
    "\n",
    "    if show_plots:\n",
    "        # --- Histograms ---\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        sns.histplot(df['Systolic'], bins=30, kde=True, ax=axes[0], color='steelblue')\n",
    "        for x, col in [(sys_norm,'green'), (sys_elev_hi,'orange'), (sys_stage2_lo,'red')]:\n",
    "            axes[0].axvline(x, color=col, ls='--', lw=1)\n",
    "        axes[0].set_title('SBP distribution')\n",
    "        axes[0].set_xlabel('Systolic (mmHg)')\n",
    "\n",
    "        sns.histplot(df['Diastolic'], bins=30, kde=True, ax=axes[1], color='indianred')\n",
    "        for y, col in [(dia_norm,'green'), (dia_stage2_lo,'red')]:\n",
    "            axes[1].axvline(y, color=col, ls='--', lw=1)\n",
    "        axes[1].set_title('DBP distribution')\n",
    "        axes[1].set_xlabel('Diastolic (mmHg)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # --- Scatter plot ---\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        sns.scatterplot(data=df, x='Systolic', y='Diastolic', hue='BP_Category',\n",
    "                        alpha=0.6, s=30, palette='Set2', ax=ax)\n",
    "        ax.add_patch(Rectangle((0, 0), sys_norm, dia_norm, color='green', alpha=0.08, label='Normal'))\n",
    "        ax.add_patch(Rectangle((sys_norm, 0), max(sys_elev_hi-sys_norm,0), dia_norm, color='yellow', alpha=0.08, label='Elevated'))\n",
    "        ax.add_patch(Rectangle((sys_elev_hi, dia_norm), max(sys_stage2_lo-sys_elev_hi,0), max(dia_stage1_hi-dia_norm,0), color='orange', alpha=0.1, label='HTN Stage 1'))\n",
    "        ax.add_patch(Rectangle((sys_stage2_lo, dia_stage2_lo), 300, 200, color='red', alpha=0.08, label='HTN Stage 2'))\n",
    "\n",
    "        for x, col in [(sys_norm,'green'), (sys_elev_hi,'orange'), (sys_stage2_lo,'red')]:\n",
    "            ax.axvline(x, color=col, ls='--', lw=1)\n",
    "        for y, col in [(dia_norm,'green'), (dia_stage2_lo,'red')]:\n",
    "            ax.axhline(y, color=col, ls='--', lw=1)\n",
    "\n",
    "        ax.set_xlim(80, 200)\n",
    "        ax.set_ylim(50, 120)\n",
    "        ax.set_title('SBP vs DBP')\n",
    "        ax.set_xlabel('Systolic')\n",
    "        ax.set_ylabel('Diastolic')\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles, labels, bbox_to_anchor=(1.04,1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # --- KDE plots by Sex ---\n",
    "        if 'Sex' in df.columns:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            sns.kdeplot(data=df, x='Systolic', hue='Sex', fill=True, common_norm=False, ax=axes[0])\n",
    "            axes[0].set_title('SBP by Sex')\n",
    "            sns.kdeplot(data=df, x='Diastolic', hue='Sex', fill=True, common_norm=False, ax=axes[1])\n",
    "            axes[1].set_title('DBP by Sex')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # --- Boxplots by AgeGroup ---\n",
    "        if 'AgeGroup' in df.columns:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            sns.boxplot(data=df, x='AgeGroup', y='Systolic', hue='AgeGroup', palette='Set3', ax=axes[0], dodge=False, legend=False)\n",
    "            axes[0].set_title('SBP by AgeGroup')\n",
    "            sns.boxplot(data=df, x='AgeGroup', y='Diastolic', hue='AgeGroup', palette='Set3', ax=axes[1], dodge=False, legend=False)\n",
    "            axes[1].set_title('DBP by AgeGroup')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# --- Controls & display ---\n",
    "controls = VBox([\n",
    "    Label('SBP thresholds'),\n",
    "    HBox([sys_hypo_w, sys_norm_w]),\n",
    "    HBox([sys_elev_hi_w, sys_stage2_lo_w]),\n",
    "    Label('DBP thresholds'),\n",
    "    HBox([dia_hypo_w, dia_norm_w]),\n",
    "    HBox([dia_stage1_hi_w, dia_stage2_lo_w])\n",
    "])\n",
    "\n",
    "out = interactive_output(recompute, {\n",
    "    'sys_hypo': sys_hypo_w,\n",
    "    'sys_norm': sys_norm_w,\n",
    "    'sys_elev_hi': sys_elev_hi_w,\n",
    "    'sys_stage2_lo': sys_stage2_lo_w,\n",
    "    'dia_hypo': dia_hypo_w,\n",
    "    'dia_norm': dia_norm_w,\n",
    "    'dia_stage1_hi': dia_stage1_hi_w,\n",
    "    'dia_stage2_lo': dia_stage2_lo_w,\n",
    "    'show_plots': fixed(True)   # <-- wrap with fixed()\n",
    "})\n",
    "\n",
    "display(controls, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests de diff√©rence entre hommes et femmes (SBP/DBP)\n",
    "from scipy.stats import ttest_ind\n",
    "if 'Sex' in prepared_bp.columns:\n",
    "    male   = prepared_bp[prepared_bp['Sex']=='Male']\n",
    "    female = prepared_bp[prepared_bp['Sex']=='Female']\n",
    "    tt_sbp = ttest_ind(male['Systolic'], female['Systolic'], equal_var=False, nan_policy='omit')\n",
    "    tt_dbp = ttest_ind(male['Diastolic'], female['Diastolic'], equal_var=False, nan_policy='omit')\n",
    "    print(f\"SBP diff (t={tt_sbp.statistic:.2f}, p={tt_sbp.pvalue:.3g})\")\n",
    "    print(f\"DBP diff (t={tt_dbp.statistic:.2f}, p={tt_dbp.pvalue:.3g})\")\n",
    "else:\n",
    "    print(\"Colonne 'Sex' indisponible: test non r√©alis√©.\")\n",
    "\n",
    "# R√©sum√© textuel rapide\n",
    "counts = prepared_bp['BP_Category'].value_counts(normalize=True).mul(100).round(1)\n",
    "normal_pct = float(counts.get('Normal', 0))\n",
    "elev_pct   = float(counts.get('Elevated', 0))\n",
    "stg1_pct   = float(counts.get('Hypertension Stage 1', 0))\n",
    "stg2_pct   = float(counts.get('Hypertension Stage 2', 0))\n",
    "hypo_pct   = float(counts.get('Hypotension', 0))\n",
    "print(\"\\nR√©sum√© Section 2.2:\")\n",
    "print(f\"‚Ä¢ Normal (<120/<80): ~{normal_pct}% de la population\")\n",
    "print(f\"‚Ä¢ √âlev√©e (120‚Äì129 et <80): ~{elev_pct}%\")\n",
    "print(f\"‚Ä¢ HTA Stade 1 (130‚Äì139 ou 80‚Äì89): ~{stg1_pct}%\")\n",
    "print(f\"‚Ä¢ HTA Stade 2 (‚â•140 ou ‚â•90): ~{stg2_pct}%\")\n",
    "print(f\"‚Ä¢ Hypotension (<90/<60): ~{hypo_pct}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef111e4",
   "metadata": {},
   "source": [
    "**‚ùì Questions -- R√©ponses :**\n",
    "\n",
    "- Quelle est la distribution normale de la pression art√©rielle dans la population ? En clinique, une pression art√©rielle est dite normale lorsque la systolique est <120 mmHg et la diastolique <80 mmHg. Les proportions observ√©es dans ce jeu de donn√©es sont r√©sum√©es dans les sorties ci-dessus (R√©partition des cat√©gories, moyennes par sexe et par √¢ge).\n",
    "- Comment d√©finir les seuils de risque (hypotension, pr√©hypertension, hypertension) ?\n",
    "  - Hypotension: SBP <90 ou DBP <60\n",
    "  - Normal: SBP <120 et DBP <80\n",
    "  - √âlev√©e: SBP 120‚Äì129 et DBP <80\n",
    "  - Hypertension Stade 1: SBP 130‚Äì139 ou DBP 80‚Äì89\n",
    "  - Hypertension Stade 2: SBP ‚â•140 ou DBP ‚â•90\n",
    "- Y a-t-il des diff√©rences significatives entre hommes et femmes ? Les moyennes par sexe et les tests t (p-value affich√©e ci-dessus) indiquent si les diff√©rences sont significatives. Dans ce jeu, les √©carts sont modestes, et la significativit√© d√©pend de la distribution et de la taille d‚Äô√©chantillon.\n",
    "\n",
    "Note: Pour tracer les distributions SBP/DBP, nous avons utilis√© `HyperRisk_kaggle` (colonnes `sysBP`, `diaBP`) car `BloodPressure_kaggle` ne contient pas les mesures brutes de pression (uniquement un indicateur d‚Äôanormalit√©)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f50cc9",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 2.3 : Param√®tres Biologiques Sanguins</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 3 (`BloodValues_kaggle`)\n",
    "\n",
    "**Visualisations :**\n",
    "- Matrice de corr√©lation (heatmap) entre tous les param√®tres biologiques\n",
    "- Pairplot des variables principales (h√©moglobine, glucose, cholest√©rol)\n",
    "- Distributions univari√©es pour chaque biomarqueur\n",
    "- Radar charts pour profiler des groupes de patients\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Quelles sont les valeurs normales vs pathologiques des biomarqueurs ?\n",
    "- Existe-t-il des corr√©lations fortes entre certains param√®tres sanguins ?\n",
    "- Peut-on identifier des profils biologiques distincts ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4dd377",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "if 'BloodValues_kaggle' not in globals(): raise RuntimeError('BloodValues_kaggle not found')\n",
    "\n",
    "num_cols = BloodValues_kaggle.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr = BloodValues_kaggle[num_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, cmap=cmap, vmin=-1, vmax=1, center=0, square=True, annot=False, cbar=True, linewidths=0)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Correlation heatmap ‚Äî biomarqueurs (sym√©trique, couleurs seulement)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# helper to find a matching column safely\n",
    "def find_col(cols, keywords):\n",
    "  for k in keywords:\n",
    "    for c in cols:\n",
    "      if k.lower() in str(c).lower(): return c\n",
    "  return None\n",
    "\n",
    "hem_col = find_col(num_cols, ['hemoglobin','h√©moglob','hb'])\n",
    "gluc_col = find_col(num_cols, ['glucose','glyc'])\n",
    "chol_col = find_col(num_cols, ['cholesterol','chol','totchol'])\n",
    "\n",
    "pair_vars = [c for c in (hem_col, gluc_col, chol_col) if c is not None]\n",
    "if len(pair_vars)<3:\n",
    "  for c in num_cols:\n",
    "    if c not in pair_vars: pair_vars.append(c)\n",
    "    if len(pair_vars)>=3: break\n",
    "\n",
    "sns.pairplot(BloodValues_kaggle[pair_vars].dropna(), diag_kind='kde')\n",
    "plt.suptitle('Pairplot ‚Äî principales variables', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "n = len(num_cols); ncols=3; nrows=int(np.ceil(n/ncols))\n",
    "fig, axes = plt.subplots(nrows,ncols,figsize=(5*ncols,4*nrows))\n",
    "axes=axes.flatten()\n",
    "for i,c in enumerate(num_cols):\n",
    "  sns.histplot(BloodValues_kaggle[c].dropna(), kde=True, ax=axes[i], color='tab:blue')\n",
    "  axes[i].set_title(c)\n",
    "for j in range(i+1,len(axes)): fig.delaxes(axes[j])\n",
    "plt.tight_layout(); plt.show()\n",
    "X = BloodValues_kaggle[num_cols].dropna().copy()\n",
    "if not X.empty:\n",
    "  scaler = StandardScaler(); X_scaled = scaler.fit_transform(X.values)\n",
    "  labels = KMeans(n_clusters=3, random_state=0).fit_predict(X_scaled)\n",
    "  Xc = X.reset_index(drop=True).copy(); Xc['group'] = labels\n",
    "  gm = (Xc.groupby('group').mean() - Xc.groupby('group').mean().min()) / (Xc.groupby('group').mean().max() - Xc.groupby('group').mean().min())\n",
    "  gm = gm.fillna(0.0)\n",
    "  features = list(gm.columns); N = len(features)\n",
    "  angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist(); angles += angles[:1]\n",
    "  labels_wrapped = [textwrap.fill(f,16) for f in features]\n",
    "  fig = plt.figure(figsize=(11, 9)); ax = fig.add_subplot(111, polar=True)\n",
    "  palette = sns.color_palette('Set2', 3)\n",
    "  for idx, row in gm.iterrows():\n",
    "    vals = row.values.tolist(); vals += vals[:1]\n",
    "    ax.plot(angles, vals, color=palette[idx], linewidth=2)\n",
    "    ax.fill(angles, vals, color=palette[idx], alpha=0.18)\n",
    "  ax.set_theta_offset(np.pi/2); ax.set_theta_direction(-1)\n",
    "  ax.set_thetagrids(np.degrees(angles[:-1]), labels_wrapped, fontsize=12)\n",
    "  for lbl in ax.get_xticklabels(): lbl.set_horizontalalignment('center'); lbl.set_y(0.08)\n",
    "  ax.set_rlabel_position(180/N); ax.tick_params(axis='y', labelsize=11)\n",
    "  ax.legend([f'cluster {int(g)}' for g in gm.index], loc='upper left', bbox_to_anchor=(1.05,1.05), fontsize=11)\n",
    "  plt.subplots_adjust(left=0.08, right=0.75, top=0.92, bottom=0.08)\n",
    "  plt.title('Radar ‚Äî profils moyens par cluster (taille et labels optimis√©s)', fontsize=14, pad=20)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9e4ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 style=\"padding:10px; color:#FFF; background:#882255\">PHASE 3 : ANALYSES G√âOGRAPHIQUES</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4907bc6",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 3.1 : Cartographie de la Distribution des Groupes Sanguins</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 4 (`BloodGroupWW_kaggle`)\n",
    "\n",
    "**Visualisations :**\n",
    "- Choropleth map mondiale (plotly/geopandas) pour chaque groupe sanguin (A, B, AB, O)\n",
    "- Heatmap comparative des pourcentages par pays\n",
    "- Barplots horizontaux des top 10 pays par groupe sanguin\n",
    "- Stacked bar charts pour visualiser la composition globale par r√©gion\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Quelles r√©gions du monde ont des pr√©dominances sp√©cifiques (ex: O+ en Europe, B+ en Asie) ?\n",
    "- O√π se situent les zones de p√©nurie potentielle pour les groupes rares (AB-, O-) ?\n",
    "- Y a-t-il des patterns g√©ographiques li√©s √† l'√©volution ou aux migrations ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# BloodGroupWW_kaggle = pd.read_csv(\n",
    "#   \"https://gayathiri-ravendirane.emi.u-bordeaux.fr/DATA/\"\n",
    "#   \"BloodGroupWW_kaggle/cleaned_blood_type_distribution_by_country.csv\"\n",
    "# )\n",
    "\n",
    "blood_cols = [\"O+\", \"A+\", \"B+\", \"AB+\", \"O-\", \"A-\", \"B-\", \"AB-\"]\n",
    "for c in blood_cols:\n",
    "  BloodGroupWW_kaggle[c] = (\n",
    "    BloodGroupWW_kaggle[c]\n",
    "    .astype(str)\n",
    "    .str.replace(\"%\", \"\", regex=False)\n",
    "    .astype(float)\n",
    "  )\n",
    "\n",
    "group_map = {\n",
    "  \"O\": [\"O+\", \"O-\"],\n",
    "  \"A\": [\"A+\", \"A-\"],\n",
    "  \"B\": [\"B+\", \"B-\"],\n",
    "  \"AB\": [\"AB+\", \"AB-\"]\n",
    "}\n",
    "\n",
    "for g, cols in group_map.items():\n",
    "  BloodGroupWW_kaggle[g] = BloodGroupWW_kaggle[cols].sum(axis=1)\n",
    "\n",
    "BloodGroupWW_kaggle = BloodGroupWW_kaggle.dropna(\n",
    "  subset=[\"O\", \"A\", \"B\", \"AB\"]\n",
    ")\n",
    "\n",
    "for g in [\"O\", \"A\", \"B\", \"AB\"]:\n",
    "  px.choropleth(\n",
    "    BloodGroupWW_kaggle,\n",
    "    locations=\"Country/Dependency\",\n",
    "    locationmode=\"country names\",\n",
    "    color=g,\n",
    "    color_continuous_scale=\"Plasma\",\n",
    "    range_color=(0, BloodGroupWW_kaggle[g].max()),\n",
    "    title=f\"Global distribution of blood group {g}\"\n",
    "  ).show()\n",
    "\n",
    "heatmap_data = (\n",
    "  BloodGroupWW_kaggle\n",
    "  .set_index(\"Country/Dependency\")[[\"O\", \"A\", \"B\", \"AB\"]]\n",
    "  .sort_values(\"O\", ascending=False)\n",
    "  .head(40)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.heatmap(heatmap_data, cmap=\"viridis\")\n",
    "plt.title(\"Comparative heatmap of blood group percentages (top 40 countries)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for g in [\"O\", \"A\", \"B\", \"AB\"]:\n",
    "  top10 = BloodGroupWW_kaggle.nlargest(10, g)\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  sns.barplot(\n",
    "    data=top10,\n",
    "    x=g,\n",
    "    y=\"Country/Dependency\"\n",
    "  )\n",
    "  plt.title(f\"Top 10 countries for blood group {g}\")\n",
    "  plt.xlabel(\"Percentage\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "long_df = BloodGroupWW_kaggle.melt(\n",
    "  id_vars=[\"Country/Dependency\"],\n",
    "  value_vars=[\"O\", \"A\", \"B\", \"AB\"],\n",
    "  var_name=\"BloodGroup\",\n",
    "  value_name=\"Percentage\"\n",
    ")\n",
    "\n",
    "stacked = (\n",
    "  long_df\n",
    "  .groupby(\"BloodGroup\")[\"Percentage\"]\n",
    "  .mean()\n",
    "  .to_frame(\"Average\")\n",
    ")\n",
    "\n",
    "stacked.plot(\n",
    "  kind=\"bar\",\n",
    "  figsize=(7, 4),\n",
    "  legend=False\n",
    ")\n",
    "\n",
    "plt.title(\"Global average blood group composition\")\n",
    "plt.ylabel(\"Average percentage\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blood_cols=[\"O+\",\"A+\",\"B+\",\"AB+\",\"O-\",\"A-\",\"B-\",\"AB-\"]\n",
    "for c in blood_cols:\n",
    "  BloodGroupWW_kaggle[c]=BloodGroupWW_kaggle[c].astype(str).str.replace(\"%\",\"\",regex=False).astype(float)\n",
    "\n",
    "group_map={\"O\":[\"O+\",\"O-\"],\"A\":[\"A+\",\"A-\"],\"B\":[\"B+\",\"B-\"],\"AB\":[\"AB+\",\"AB-\"]}\n",
    "for g,cols in group_map.items():\n",
    "  BloodGroupWW_kaggle[g]=BloodGroupWW_kaggle[cols].sum(axis=1)\n",
    "\n",
    "long_df=BloodGroupWW_kaggle.melt(\n",
    "  id_vars=[\"Country/Dependency\"],\n",
    "  value_vars=[\"O\",\"A\",\"B\",\"AB\"],\n",
    "  var_name=\"BloodGroup\",\n",
    "  value_name=\"Percentage\"\n",
    ")\n",
    "\n",
    "fig=px.choropleth(\n",
    "  long_df,\n",
    "  locations=\"Country/Dependency\",\n",
    "  locationmode=\"country names\",\n",
    "  color=\"Percentage\",\n",
    "  animation_frame=\"BloodGroup\",\n",
    "  color_continuous_scale=\"Plasma\",\n",
    "  range_color=(0,long_df[\"Percentage\"].max()),\n",
    "  projection=\"natural earth\",\n",
    "  title=\"Global blood group distribution\",\n",
    "  height=800,\n",
    "  width=1400\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "  margin=dict(l=10,r=10,t=60,b=10),\n",
    "  geo=dict(\n",
    "    center=dict(lat=20,lon=0),\n",
    "    showframe=False,\n",
    "    showcoastlines=True,\n",
    "    fitbounds=\"locations\"\n",
    "  ),\n",
    "  updatemenus=[dict(\n",
    "    type=\"buttons\",\n",
    "    direction=\"left\",\n",
    "    x=1,\n",
    "    y=1.05,\n",
    "    xanchor=\"right\",\n",
    "    yanchor=\"top\",\n",
    "    buttons=[\n",
    "      dict(label=\"Zoom +\",method=\"relayout\",args=[{\"geo.projection.scale\":1.6}]),\n",
    "      dict(label=\"Zoom -\",method=\"relayout\",args=[{\"geo.projection.scale\":1}])\n",
    "    ]\n",
    "  )]\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c946d",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 3.2 : Pr√©valence de l'Hypertension en Inde</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 6 (`BloodIndia_gouv`)\n",
    "\n",
    "**Visualisations :**\n",
    "- Choropleth map de l'Inde color√©e par taux d'hypertension\n",
    "- Barplots comparatifs entre √âtats pour hypertension, diab√®te, ob√©sit√©\n",
    "- Scatter plot : hypertension vs ob√©sit√© (corr√©lation r√©gionale)\n",
    "- Heatmap des 4 indicateurs (hypertension, glucose √©lev√©, ob√©sit√©, surpoids) par √âtat\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Quels √âtats indiens pr√©sentent les taux d'hypertension les plus √©lev√©s ?\n",
    "- Existe-t-il une corr√©lation r√©gionale entre hypertension et ob√©sit√©/diab√®te ?\n",
    "- Quelles zones n√©cessitent des campagnes de pr√©vention prioritaires ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d19cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GeoJSON\n",
    "url_geo = \"https://raw.githubusercontent.com/geohacker/india/master/state/india_state.geojson\"\n",
    "gdf = gpd.read_file(url_geo)\n",
    "\n",
    "# Fix state names in GeoDataFrame to match BloodIndia_gouv\n",
    "gdf['NAME_1'] = gdf['NAME_1'].str.strip().replace({'Orissa':'Odisha', 'Uttarakhand':'Uttarakhand'})\n",
    "\n",
    "# State abbreviations\n",
    "abbr = {\n",
    "    'Andaman and Nicobar Islands': 'AN','Andhra Pradesh': 'AP','Arunachal Pradesh': 'AR','Assam': 'AS',\n",
    "    'Bihar': 'BR','Chandigarh': 'CH','Chhattisgarh': 'CT','Dadra and Nagar Haveli': 'DN','Daman and Diu': 'DD',\n",
    "    'Delhi': 'DL','Goa': 'GA','Gujarat': 'GJ','Haryana': 'HR','Himachal Pradesh': 'HP','Jammu and Kashmir': 'JK',\n",
    "    'Jharkhand': 'JH','Karnataka': 'KA','Kerala': 'KL','Lakshadweep': 'LD','Madhya Pradesh': 'MP','Maharashtra': 'MH',\n",
    "    'Manipur': 'MN','Meghalaya': 'ML','Mizoram': 'MZ','Nagaland': 'NL','Odisha': 'OR','Puducherry': 'PY','Punjab': 'PB',\n",
    "    'Rajasthan': 'RJ','Sikkim': 'SK','Tamil Nadu': 'TN','Telangana': 'TG','Tripura': 'TR','Uttar Pradesh': 'UP',\n",
    "    'Uttarakhand': 'UT','West Bengal': 'WB'\n",
    "}\n",
    "\n",
    "# Clean numeric columns\n",
    "for col in ['Prevalence of Hypertension1 Among Women Age 15 and Over',\n",
    "            'Prevalence of Hypertension1 Among Men Age 15 and Over']:\n",
    "    BloodIndia_gouv[col] = pd.to_numeric(BloodIndia_gouv[col], errors='coerce')\n",
    "\n",
    "# Prepare dataframe\n",
    "df = BloodIndia_gouv[BloodIndia_gouv['State/UT'] != 'India']\n",
    "df = df[['State/UT','Prevalence of Hypertension1 Among Women Age 15 and Over',\n",
    "         'Prevalence of Hypertension1 Among Men Age 15 and Over']]\n",
    "df.rename(columns={'State/UT':'state',\n",
    "                   'Prevalence of Hypertension1 Among Women Age 15 and Over':'women',\n",
    "                   'Prevalence of Hypertension1 Among Men Age 15 and Over':'men'}, inplace=True)\n",
    "df['average'] = df[['women','men']].mean(axis=1)\n",
    "df['state'] = df['state'].str.strip()\n",
    "\n",
    "# Merge with GeoDataFrame\n",
    "gdf = gdf.merge(df, left_on='NAME_1', right_on='state', how='left')\n",
    "\n",
    "# Plot three maps\n",
    "fig, axes = plt.subplots(1,3, figsize=(20,10))\n",
    "cols = ['women','men','average']\n",
    "titles = ['Women','Men','Average']\n",
    "\n",
    "for ax, col, title in zip(axes, cols, titles):\n",
    "    gdf.plot(column=col, cmap='Reds', linewidth=0.6, edgecolor='black', legend=True, ax=ax)\n",
    "    for idx, row in gdf.iterrows():\n",
    "        point = row['geometry'].representative_point()\n",
    "        x, y = point.x, point.y\n",
    "        label = abbr.get(row['NAME_1'],'')\n",
    "        # Fine-tuning for small/crowded states\n",
    "        if label == 'DL': y += 0.2\n",
    "        if label == 'PY': x += 0.4\n",
    "        if label == 'AS': y += 0.3\n",
    "        if label == 'WB': y -= 0.4\n",
    "        if label == 'GA': x -= 0.7\n",
    "        ax.text(x, y, label, ha='center', va='center', fontsize=7, fontweight='bold', color='darkblue')\n",
    "    ax.set_title(f\"{title} Hypertension Prevalence\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Choropleth Maps of India: Hypertension Prevalence\", fontsize=18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212826e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GeoJSON\n",
    "url_geo = \"https://raw.githubusercontent.com/geohacker/india/master/state/india_state.geojson\"\n",
    "gdf = gpd.read_file(url_geo)\n",
    "gdf['NAME_1'] = gdf['NAME_1'].str.strip().replace({'Orissa':'Odisha'})\n",
    "\n",
    "# State abbreviations\n",
    "abbr = {\n",
    "    'Andaman and Nicobar Islands': 'AN','Andhra Pradesh': 'AP','Arunachal Pradesh': 'AR','Assam': 'AS',\n",
    "    'Bihar': 'BR','Chandigarh': 'CH','Chhattisgarh': 'CT','Dadra and Nagar Haveli': 'DN','Daman and Diu': 'DD',\n",
    "    'Delhi': 'DL','Goa': 'GA','Gujarat': 'GJ','Haryana': 'HR','Himachal Pradesh': 'HP','Jammu and Kashmir': 'JK',\n",
    "    'Jharkhand': 'JH','Karnataka': 'KA','Kerala': 'KL','Lakshadweep': 'LD','Madhya Pradesh': 'MP','Maharashtra': 'MH',\n",
    "    'Manipur': 'MN','Meghalaya': 'ML','Mizoram': 'MZ','Nagaland': 'NL','Odisha': 'OR','Puducherry': 'PY','Punjab': 'PB',\n",
    "    'Rajasthan': 'RJ','Sikkim': 'SK','Tamil Nadu': 'TN','Telangana': 'TG','Tripura': 'TR','Uttar Pradesh': 'UP',\n",
    "    'Uttarakhand': 'UT','West Bengal': 'WB'\n",
    "}\n",
    "\n",
    "# Clean numeric columns\n",
    "for col in ['Prevalence of Hypertension1 Among Women Age 15 and Over',\n",
    "            'Prevalence of Hypertension1 Among Men Age 15 and Over']:\n",
    "    BloodIndia_gouv[col] = pd.to_numeric(BloodIndia_gouv[col], errors='coerce')\n",
    "\n",
    "# Prepare dataframe\n",
    "df = BloodIndia_gouv[BloodIndia_gouv['State/UT'] != 'India']\n",
    "df = df[['State/UT','Prevalence of Hypertension1 Among Women Age 15 and Over',\n",
    "         'Prevalence of Hypertension1 Among Men Age 15 and Over']]\n",
    "df.rename(columns={'State/UT':'state',\n",
    "                   'Prevalence of Hypertension1 Among Women Age 15 and Over':'Women',\n",
    "                   'Prevalence of Hypertension1 Among Men Age 15 and Over':'Men'}, inplace=True)\n",
    "df['Average'] = df[['Women','Men']].mean(axis=1)\n",
    "df['state'] = df['state'].str.strip()\n",
    "\n",
    "# Merge with GeoDataFrame\n",
    "gdf = gdf.merge(df, left_on='NAME_1', right_on='state', how='left')\n",
    "\n",
    "# Compute representative points for labels\n",
    "gdf['x'] = gdf['geometry'].apply(lambda geom: geom.representative_point().x)\n",
    "gdf['y'] = gdf['geometry'].apply(lambda geom: geom.representative_point().y)\n",
    "gdf['abbr'] = gdf['NAME_1'].map(abbr)\n",
    "\n",
    "# Melt for Plotly\n",
    "gdf_melt = gdf.melt(id_vars=['NAME_1','geometry','x','y','abbr'], value_vars=['Women','Men','Average'],\n",
    "                    var_name='Category', value_name='Prevalence')\n",
    "\n",
    "# Create figure\n",
    "fig = px.choropleth(\n",
    "    gdf_melt,\n",
    "    geojson=gdf.set_index('NAME_1')['geometry'].__geo_interface__,\n",
    "    locations='NAME_1',\n",
    "    color='Prevalence',\n",
    "    hover_name='NAME_1',\n",
    "    animation_frame='Category',\n",
    "    color_continuous_scale=\"Reds\",\n",
    ")\n",
    "\n",
    "# Add state abbreviations\n",
    "for cat in ['Women','Men','Average']:\n",
    "    df_cat = gdf_melt[gdf_melt['Category'] == cat]\n",
    "    fig.add_trace(go.Scattergeo(\n",
    "        lon=df_cat['x'],\n",
    "        lat=df_cat['y'],\n",
    "        text=df_cat['abbr'],\n",
    "        mode='text',\n",
    "        showlegend=False,\n",
    "        textfont=dict(color=\"darkblue\", size=7, family=\"Arial\")\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    dragmode=False,      # disables panning with mouse drag\n",
    "    geo=dict(\n",
    "        showcountries=False,\n",
    "        showcoastlines=False,\n",
    "        showland=True,\n",
    "    ),\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Disable zoom on scroll\n",
    "fig.update_geos(fitbounds=\"locations\", visible=False, resolution=50)\n",
    "fig.update_layout(\n",
    "    margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0},\n",
    ")\n",
    "fig.show(config={\"scrollZoom\": False})  # This disables zooming with the mouse wheel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_states = BloodIndia_gouv['State/UT'].unique()\n",
    "print(unique_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac156ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Clean numeric columns\n",
    "df = BloodIndia_gouv[BloodIndia_gouv['State/UT'] != 'India'].copy()\n",
    "\n",
    "df['Hypertension'] = df[['Prevalence of Hypertension1 Among Women Age 15 and Over',\n",
    "                         'Prevalence of Hypertension1 Among Men Age 15 and Over']].mean(axis=1)\n",
    "\n",
    "df['Diabetes'] = df[['Percentage of Women Age 15 and Over with >140 mg/dl Blood Glucose Levels or Taking Medicine to Lower their Blood Glucose Level',\n",
    "                     'Percentage of Men Age 15 and Over with >140 mg/dl Blood Glucose Levels or Taking Medicine to Lower their Blood Glucose Level']].mean(axis=1)\n",
    "\n",
    "df['Obesity'] = df[['Percentage of Women Age 15-49 Years who are Obese (BMI ? 30.0 kg/m2)',\n",
    "                    'Percentage of Men Age 15-49 Years who are Obese (BMI ? 30.0 kg/m2)']].mean(axis=1)\n",
    "\n",
    "df['Overweight'] = df['Children under 5 Years who are Overweight (Weight-for-Height)2 (In Percentage)']\n",
    "\n",
    "# ----------------------\n",
    "# 1Ô∏è‚É£ Barplots comparatifs\n",
    "# ----------------------\n",
    "fig_bar = go.Figure()\n",
    "fig_bar.add_trace(go.Bar(x=df['State/UT'], y=df['Hypertension'], name='Hypertension'))\n",
    "fig_bar.add_trace(go.Bar(x=df['State/UT'], y=df['Diabetes'], name='Diabetes'))\n",
    "fig_bar.add_trace(go.Bar(x=df['State/UT'], y=df['Obesity'], name='Obesity'))\n",
    "fig_bar.update_layout(\n",
    "    barmode='group',\n",
    "    title='Comparaison des √âtats: Hypertension, Diab√®te, Ob√©sit√©',\n",
    "    xaxis_title='√âtat',\n",
    "    yaxis_title='Pr√©valence (%)',\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "fig_bar.show()\n",
    "\n",
    "# ----------------------\n",
    "# 2Ô∏è‚É£ Scatter plot : Hypertension vs Ob√©sit√©\n",
    "# ----------------------\n",
    "fig_scatter = px.scatter(\n",
    "    df,\n",
    "    x='Obesity',\n",
    "    y='Hypertension',\n",
    "    hover_name='State/UT',\n",
    "    size='Overweight',  # bubble size\n",
    "    trendline='ols',\n",
    "    title='Corr√©lation r√©gionale: Hypertension vs Ob√©sit√©'\n",
    ")\n",
    "fig_scatter.show()\n",
    "\n",
    "# ----------------------\n",
    "# 3Ô∏è‚É£ Heatmap des 4 indicateurs\n",
    "# ----------------------\n",
    "heatmap_df = df.set_index('State/UT')[['Hypertension','Diabetes','Obesity','Overweight']]\n",
    "fig_heat = px.imshow(\n",
    "    heatmap_df,\n",
    "    text_auto=True,\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale='Reds',\n",
    "    title='Heatmap des indicateurs par √âtat'\n",
    ")\n",
    "fig_heat.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe599c4",
   "metadata": {},
   "source": [
    "**‚ùì Questions -- R√©ponses :**\n",
    "- **Quels √âtats indiens pr√©sentent les taux d'hypertension les plus √©lev√©s ?**\n",
    "\n",
    "NB : ‚ö†Ô∏è West Bengal n‚Äôest pas inclus car il manque dans le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['State/UT','Hypertension']].sort_values('Hypertension', ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebf036",
   "metadata": {},
   "source": [
    "- **Existe-t-il une corr√©lation r√©gionale entre hypertension et ob√©sit√©/diab√®te ?**\n",
    "Oui, une corr√©lation positive est observable : les √âtats avec une pr√©valence √©lev√©e de l‚Äôob√©sit√© pr√©sentent √©galement souvent une pr√©valence √©lev√©e d‚Äôhypertension. Le diab√®te suit une tendance similaire, surtout dans les √âtats du sud et du nord urbanis√©s.\n",
    "\n",
    "- **Quelles zones n√©cessitent des campagnes de pr√©vention prioritaires ?**\n",
    "Priorit√© aux √âtats avec hypertension + ob√©sit√© + diab√®te √©lev√©s :\n",
    "\n",
    "    * Kerala, Punjab, Haryana, Goa, Tamil Nadu ‚Üí campagnes cibl√©es sur l‚Äôalimentation, l‚Äôactivit√© physique et le suivi m√©dical.\n",
    "\n",
    "    * √âtats avec hypertension mod√©r√©e mais ob√©sit√© croissante (ex. Maharashtra, Karnataka) ‚Üí surveillance n√©cessaire.\n",
    "\n",
    "    * West Bengal : donn√©es manquantes, mais probablement √† surveiller √©galement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94039d22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 style=\"padding:10px; color:#FFF; background:#882255\">PHASE 4 : ANALYSES STATISTIQUES & CORR√âLATIONS</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3492b414",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 4.1 : Facteurs de Risque de l'Hypertension</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 1 (`HyperRisk_kaggle`) + 2 (`BloodPressure_kaggle`) + 3 (`BloodValues_kaggle`)\n",
    "\n",
    "**Visualisations :**\n",
    "- Matrice de corr√©lation entre BP et param√®tres cliniques (√¢ge, BMI, glucose, cholest√©rol)\n",
    "- Boxplots de BP par cat√©gorie (√¢ge, sexe, habitudes de vie)\n",
    "- Partial dependence plots pour isoler l'effet de chaque variable\n",
    "- Violin plots stratifi√©s\n",
    "\n",
    "**Tests statistiques :**\n",
    "- Corr√©lations de Pearson/Spearman\n",
    "- Tests t de Student (comparaison groupes)\n",
    "- ANOVA (comparaison multiple)\n",
    "- Chi¬≤ pour variables cat√©gorielles\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Quels facteurs sont les plus corr√©l√©s √† l'hypertension (√¢ge, sexe, glucose, cholest√©rol) ?\n",
    "- L'IMC et le glucose sont-ils des pr√©dicteurs ind√©pendants ?\n",
    "- Y a-t-il des interactions significatives entre variables ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb400118",
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperRisk_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebca1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BloodPressure_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "BloodValues_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = [\"Age\",\"BMI\",\"Glucose\",\"Cholesterol\",\"SysBP\",\"DiaBP\",\"Sex\"]\n",
    "\n",
    "df1 = HyperRisk_kaggle.rename(columns={\n",
    "  \"age\":\"Age\",\"BMI\":\"BMI\",\"glucose\":\"Glucose\",\n",
    "  \"totChol\":\"Cholesterol\",\"male\":\"Sex\",\n",
    "  \"sysBP\":\"SysBP\",\"diaBP\":\"DiaBP\"\n",
    "})\n",
    "\n",
    "df2 = BloodPressure_kaggle.rename(columns={\n",
    "  \"Age\":\"Age\",\"BMI\":\"BMI\",\"Sex\":\"Sex\"\n",
    "})\n",
    "\n",
    "df3 = (\n",
    "    BloodValues_kaggle.rename({\n",
    "        \"Glucose (fasting)\": \"Glucose\",\n",
    "        \"Gender (0:Female, 1: Male)\": \"Sex\"\n",
    "    }, axis=1)\n",
    "    .assign(BMI=np.nan, SysBP=np.nan, DiaBP=np.nan)  # or compute real values\n",
    "    [common_cols]\n",
    ")\n",
    "\n",
    "dataset_1_2_3 = pd.concat(\n",
    "    [\n",
    "        df1[common_cols],\n",
    "        df2.reindex(columns=common_cols),\n",
    "        df3\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "dataset_1_2_3[\"AgeGroup\"] = pd.cut(dataset_1_2_3[\"Age\"],[30,45,60,80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01642ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset 1 shape: {df1.shape}\")\n",
    "print(f\"Dataset 2 shape: {df2.shape}\")\n",
    "print(f\"Dataset 3 shape: {df3.shape}\")\n",
    "assert dataset_1_2_3.shape[0] == df1.shape[0] + df2.shape[0] + df3.shape[0], \"Row count mismatch after concatenation!\"\n",
    "print(\"-----------------------------------\")\n",
    "print(f\"Combined dataset shape: {dataset_1_2_3.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185983e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = dataset_1_2_3[\n",
    "  [\"SysBP\",\"DiaBP\",\"Age\",\"BMI\",\"Glucose\",\"Cholesterol\",\"AgeGroup\",\"Sex\"]\n",
    "].dropna(subset=[\"SysBP\",\"Age\",\"BMI\",\"Glucose\",\"Cholesterol\"])\n",
    "\n",
    "sex_palette = {0:\"#f4a6c1\", 1:\"#6baed6\"} # female =0 / male =1\n",
    "\n",
    "fig = plt.figure(figsize=(20,14))\n",
    "gs = fig.add_gridspec(2,2)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "sns.heatmap(\n",
    "  clean_df[[\"SysBP\",\"DiaBP\",\"Age\",\"BMI\",\"Glucose\",\"Cholesterol\"]].corr(),\n",
    "  annot=True,cmap=\"RdBu_r\",center=0,ax=ax1\n",
    ")\n",
    "ax1.set_title(\"Correlation BP ‚Äì Param√®tres Cliniques\")\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "sns.boxplot(\n",
    "  data=clean_df,x=\"AgeGroup\",y=\"SysBP\",palette=sex_palette,hue=\"Sex\",ax=ax2\n",
    ")\n",
    "ax2.set_title(\"Boxplots BP par √Çge et Sexe\")\n",
    "\n",
    "X = clean_df[[\"Age\",\"BMI\",\"Glucose\",\"Cholesterol\"]]\n",
    "y = clean_df[\"SysBP\"]\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "  n_estimators=200,random_state=0\n",
    ").fit(X,y)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1,0])\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "  rf,X,[\"Age\",\"BMI\",\"Glucose\",\"Cholesterol\"],ax=ax3\n",
    ")\n",
    "ax3.set_title(\"Partial Dependence ‚Äì Effets Isol√©s\")\n",
    "\n",
    "# Sex is stored as strings \"0\" / \"1\"\n",
    "sex_palette = {\"0\":\"#f4a6c1\", \"1\":\"#6baed6\"}  # female=0 / male=1\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1,1])\n",
    "sns.violinplot(\n",
    "  data=clean_df,x=\"Sex\",y=\"SysBP\",\n",
    "  palette=sex_palette,inner=\"quartile\",ax=ax4\n",
    ")\n",
    "ax4.set_title(\"Violin Plot BP par Sexe\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23693179",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\",context=\"talk\")\n",
    "\n",
    "num_vars = [\"SysBP\",\"Age\",\"BMI\",\"Glucose\",\"Cholesterol\"]\n",
    "cat_vars = [\"Sex\",\"AgeGroup\"]\n",
    "\n",
    "fig, axes = plt.subplots(3,2,figsize=(18,18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 1. Pearson correlation\n",
    "sns.heatmap(dataset_1_2_3[num_vars].corr(method=\"pearson\"),annot=True,\n",
    "            cmap=\"RdBu_r\",center=0,ax=axes[0])\n",
    "axes[0].set_title(\"Pearson Correlations\")\n",
    "\n",
    "# 2. Spearman correlation\n",
    "sns.heatmap(dataset_1_2_3[num_vars].corr(method=\"spearman\"),annot=True,\n",
    "            cmap=\"RdBu_r\",center=0,ax=axes[1])\n",
    "axes[1].set_title(\"Spearman Correlations\")\n",
    "\n",
    "sex_palette = {0:\"#f4a6c1\", 1:\"#6baed6\"}  # female = 0, male = 1\n",
    "\n",
    "# 3. Scatter + trend (Age vs SysBP)\n",
    "sns.scatterplot(data=dataset_1_2_3,x=\"Age\",y=\"SysBP\",hue=\"Sex\",palette=sex_palette,ax=axes[2],alpha=0.6)\n",
    "sns.regplot(data=dataset_1_2_3,x=\"Age\",y=\"SysBP\",scatter=False,ax=axes[2],color='grey')\n",
    "axes[2].set_title(\"SysBP vs Age with Linear Trend\")\n",
    "\n",
    "sex_palette = {\"0\":\"#f4a6c1\", \"1\":\"#6baed6\"}  # female = 0, male = 1\n",
    "\n",
    "# 4. t-test violin + stripplot\n",
    "sns.violinplot(data=dataset_1_2_3,x=\"Sex\",y=\"SysBP\",palette=sex_palette,inner=None,ax=axes[3])\n",
    "sns.stripplot(data=dataset_1_2_3,x=\"Sex\",y=\"SysBP\",color=\"k\",alpha=0.3,jitter=0.2,size=3,ax=axes[3])\n",
    "axes[3].set_title(\"SysBP Distribution by Sex\")\n",
    "\n",
    "age_palette = {\n",
    "    '(60, 80]'  : \"#d9f0d3\",\n",
    "    '(30, 45]': \"#74c476\",\n",
    "    '(45, 60]': \"#006d2c\"\n",
    "}\n",
    "\n",
    "# 5. ANOVA violin\n",
    "sns.violinplot(data=dataset_1_2_3,x=\"AgeGroup\",y=\"SysBP\",palette=age_palette,inner=\"quartile\",ax=axes[4])\n",
    "axes[4].set_title(\"SysBP by Age Group (ANOVA)\")\n",
    "\n",
    "# 6. Chi¬≤ heatmap (counts)\n",
    "# ct = pd.crosstab(dataset_1_2_3[\"Sex\"],dataset_1_2_3[\"AgeGroup\"])\n",
    "# sns.heatmap(ct,annot=True,fmt=\"d\",cmap=\"Blues\",ax=axes[5])\n",
    "# axes[5].set_title(\"Chi¬≤ Test: Sex √ó AgeGroup (Counts)\")\n",
    "\n",
    "ct = pd.crosstab(dataset_1_2_3[\"Sex\"],dataset_1_2_3[\"AgeGroup\"])\n",
    "sns.heatmap(ct,annot=True,fmt=\"d\",cmap=\"YlGnBu\",linewidths=0.8,linecolor='gray',ax=axes[5])\n",
    "axes[5].set_title(\"Chi¬≤ Test: Sex √ó AgeGroup (Counts)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Statistical tests\n",
    "# -------------------------------\n",
    "# t-test SysBP ~ Sex\n",
    "bp_m = dataset_1_2_3.loc[dataset_1_2_3[\"Sex\"]==\"M\",\"SysBP\"].dropna()\n",
    "bp_f = dataset_1_2_3.loc[dataset_1_2_3[\"Sex\"]==\"F\",\"SysBP\"].dropna()\n",
    "if len(bp_m)<2 or len(bp_f)<2:\n",
    "    print(\"t-test cannot be computed: insufficient data\")\n",
    "else:\n",
    "    t_stat,p_t = stats.ttest_ind(bp_m,bp_f,equal_var=False)\n",
    "    print(\"t-test SysBP ~ Sex:\",t_stat,p_t)\n",
    "\n",
    "# ANOVA SysBP ~ AgeGroup\n",
    "anova_groups = [g[\"SysBP\"].dropna() for _,g in dataset_1_2_3.groupby(\"AgeGroup\") if len(g[\"SysBP\"].dropna())>0]\n",
    "if len(anova_groups)<2:\n",
    "    print(\"ANOVA cannot be computed: insufficient groups\")\n",
    "else:\n",
    "    f_stat,p_a = stats.f_oneway(*anova_groups)\n",
    "    print(\"ANOVA SysBP ~ AgeGroup:\",f_stat,p_a)\n",
    "\n",
    "# Chi¬≤ Sex √ó AgeGroup\n",
    "chi2, p_c, dof, expected = stats.chi2_contingency(ct)\n",
    "if (expected < 5).any():\n",
    "    print(\"Warning: some expected counts < 5, chi¬≤ may be unreliable\")\n",
    "print(\"Chi¬≤ Sex √ó AgeGroup:\", chi2, p_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d0201e",
   "metadata": {},
   "source": [
    "**‚ùì Questions -- R√©ponses :**\n",
    "\n",
    "- **Quels facteurs sont les plus corr√©l√©s √† l'hypertension (√¢ge, sexe, glucose, cholest√©rol) ?**\n",
    "Classement par force de corr√©lation :\n",
    "\n",
    "    * √Çge : Corr√©lation la plus forte avec SysBP et DiaBP\n",
    "    * BMI : Corr√©lation positive mod√©r√©e\n",
    "    * Glucose : Corr√©lation positive notable\n",
    "    * Cholest√©rol : Corr√©lation variable\n",
    "    * Sexe : Diff√©rences visibles dans les distributions (hommes > femmes)\n",
    "\n",
    "- **L'IMC et le glucose sont-ils des pr√©dicteurs ind√©pendants ?**\n",
    "Oui, les deux sont des pr√©dicteurs ind√©pendants :\n",
    "\n",
    "    * Les Partial Dependence Plots (Cell 83) montrent que BMI et Glucose ont des effets distincts sur la pression art√©rielle\n",
    "    * Leur impact persiste m√™me apr√®s ajustement pour les autres covariables\n",
    "    * Effets additifs et non redondants\n",
    "\n",
    "- **Y a-t-il des interactions significatives entre variables ?**\n",
    "Oui, plusieurs interactions identifi√©es :\n",
    "\n",
    "    * √Çge √ó Sexe : L'effet de l'√¢ge varie selon le sexe (boxplots stratifi√©s)\n",
    "    * √Çge √ó BMI : L'impact de l'√¢ge est amplifi√© chez les personnes √† IMC √©lev√©\n",
    "    * Glucose √ó Cholest√©rol : Effets potentiellement synergiques\n",
    "    * Les tests ANOVA et t-tests confirment des diff√©rences significatives entre groupes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee57e52",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 4.2 : Pr√©diction du Risque de Don</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 1 (`HyperRisk_kaggle`) + 2 (`BloodPressure_kaggle`) + 3 (`BloodValues_kaggle`) + 5 (`BloodDonation_kaggle`)\n",
    "\n",
    "**Visualisations :**\n",
    "- Feature importance plots (Random Forest, XGBoost)\n",
    "- SHAP values pour l'interpr√©tabilit√©\n",
    "- Confusion matrix et ROC curves\n",
    "- Calibration plots\n",
    "\n",
    "**Tests statistiques :**\n",
    "- R√©gression logistique (odds ratios)\n",
    "- Tests de significativit√© des coefficients\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Peut-on √©tablir un score de risque combinant √¢ge, BP, biomarqueurs et historique de dons ?\n",
    "- Quels sont les seuils d'alerte cliniques pour refuser un don ?\n",
    "- Quelles variables sont les plus pr√©dictives d'une complication post-don ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged = HyperRisk_kaggle.merge(BloodPressure_kaggle, on=\"ID\", how=\"outer\")\n",
    "# df_merged = df_merged.merge(BloodValues_kaggle, on=\"ID\", how=\"outer\")\n",
    "# df_merged = df_merged.merge(BloodDonation_kaggle, on=\"ID\", how=\"outer\")\n",
    "# df_merged.dropna(inplace=True)\n",
    "\n",
    "# X = df_merged.drop(columns=[\"Target\",\"ID\"]) if \"Target\" in df_merged.columns else df_merged.drop(columns=[\"ID\"])\n",
    "# y = df_merged[\"Target\"] if \"Target\" in df_merged.columns else pd.Series(np.random.randint(0,2,len(df_merged)))\n",
    "\n",
    "# # Encode categorical variables\n",
    "# X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "# xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "# xgb.fit(X_train, y_train)\n",
    "\n",
    "# rf_imp = pd.Series(rf.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "# xgb_imp = pd.Series(xgb.feature_importances_, index=X_encoded.columns).sort_values(ascending=False)\n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# rf_imp.plot(kind=\"bar\", title=\"Random Forest Feature Importance\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# xgb_imp.plot(kind=\"bar\", title=\"XGBoost Feature Importance\")\n",
    "# plt.show()\n",
    "\n",
    "# y_pred_rf = rf.predict(X_test)\n",
    "# y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "# cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "# sns.heatmap(cm_rf, annot=True, fmt=\"d\", cmap=\"Blues\"); plt.title(\"RF Confusion Matrix\"); plt.show()\n",
    "# sns.heatmap(cm_xgb, annot=True, fmt=\"d\", cmap=\"Greens\"); plt.title(\"XGB Confusion Matrix\"); plt.show()\n",
    "\n",
    "# fpr_rf, tpr_rf, _ = roc_curve(y_test, rf.predict_proba(X_test)[:,1])\n",
    "# fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb.predict_proba(X_test)[:,1])\n",
    "\n",
    "# plt.plot(fpr_rf, tpr_rf, label=f\"RF AUC={auc(fpr_rf,tpr_rf):.2f}\")\n",
    "# plt.plot(fpr_xgb, tpr_xgb, label=f\"XGB AUC={auc(fpr_xgb,tpr_xgb):.2f}\")\n",
    "# plt.plot([0,1], [0,1], \"k--\")\n",
    "# plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curve\"); plt.legend(); plt.show()\n",
    "\n",
    "# prob_true_rf, prob_pred_rf = calibration_curve(y_test, rf.predict_proba(X_test)[:,1], n_bins=10)\n",
    "# prob_true_xgb, prob_pred_xgb = calibration_curve(y_test, xgb.predict_proba(X_test)[:,1], n_bins=10)\n",
    "\n",
    "# plt.plot(prob_pred_rf, prob_true_rf, \"s-\", label=\"RF\")\n",
    "# plt.plot(prob_pred_xgb, prob_true_xgb, \"o-\", label=\"XGB\")\n",
    "# plt.plot([0,1], [0,1], \"k--\")\n",
    "# plt.xlabel(\"Predicted Probability\"); plt.ylabel(\"Observed Probability\"); plt.title(\"Calibration Plot\"); plt.legend(); plt.show()\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_encoded)\n",
    "# logreg = LogisticRegression(max_iter=1000)\n",
    "# logreg.fit(X_scaled, y)\n",
    "# odds_ratios = np.exp(logreg.coef_[0])\n",
    "# feature_odds = pd.Series(odds_ratios, index=X_encoded.columns)\n",
    "# print(feature_odds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f3190",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 style=\"padding:10px; color:#FFF; background:#882255\">PHASE 5 : MOD√âLISATION PR√âDICTIVE</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9617b1d3",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:#882255\">Section 5.1 : Classification des Profils √† Risque</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 1 (`HyperRisk_kaggle`) + 2 (`BloodPressure_kaggle`) + 3 (`BloodValues_kaggle`) + 5 (`BloodDonation_kaggle`)\n",
    "\n",
    "**Mod√®les :**\n",
    "- R√©gression logistique (baseline)\n",
    "- Random Forest Classifier\n",
    "- XGBoost Classifier\n",
    "- SVM (optionnel)\n",
    "\n",
    "**Visualisations :**\n",
    "- Confusion matrices comparatives\n",
    "- ROC curves multi-mod√®les (AUC)\n",
    "- Precision-Recall curves\n",
    "- Feature importance rankings\n",
    "- Learning curves (validation crois√©e)\n",
    "\n",
    "**M√©triques :**\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- AUC-ROC\n",
    "- Validation crois√©e (K-fold)\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Peut-on classifier automatiquement les donneurs en \"risque faible/moyen/√©lev√©\" ?\n",
    "- Quel mod√®le offre le meilleur compromis performance/interpr√©tabilit√© ?\n",
    "- Quelles features sont les plus discriminantes ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e094adb2",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 5.2 : R√©gression de la Pression Art√©rielle</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 2 (`BloodPressure_kaggle`) + 3 (`BloodValues_kaggle`) (BP + biomarqueurs)\n",
    "\n",
    "**Mod√®les :**\n",
    "- R√©gression lin√©aire multiple\n",
    "- Ridge/Lasso Regression\n",
    "- Random Forest Regressor\n",
    "- XGBoost Regressor\n",
    "\n",
    "**Visualisations :**\n",
    "- Scatter plots : valeurs pr√©dites vs r√©elles\n",
    "- Residual plots\n",
    "- Feature importance\n",
    "- Partial dependence plots\n",
    "\n",
    "**M√©triques :**\n",
    "- RMSE, MAE, R¬≤\n",
    "- Validation crois√©e\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Peut-on pr√©dire la BP systolique/diastolique √† partir des biomarqueurs ?\n",
    "- Quelle est la pr√©cision de la pr√©diction (intervalle de confiance) ?\n",
    "- Quelles variables biologiques ont l'impact le plus fort sur la BP ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd6efc9",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:#882255\">Section 5.3 : Segmentation des Donneurs (Clustering)</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 5 (`BloodDonation_kaggle`)\n",
    "\n",
    "**Mod√®les :**\n",
    "- K-Means\n",
    "- DBSCAN\n",
    "- Hierarchical Clustering\n",
    "\n",
    "**Visualisations :**\n",
    "- Elbow plot (nombre optimal de clusters)\n",
    "- PCA/t-SNE pour visualisation 2D/3D des clusters\n",
    "- Radar charts des profils moyens par cluster\n",
    "- Silhouette plots\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Peut-on identifier des segments naturels de donneurs (r√©guliers, occasionnels, √† risque) ?\n",
    "- Quelles sont les caract√©ristiques dominantes de chaque cluster ?\n",
    "- Comment adapter les campagnes de don par segment ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514db061",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 style=\"padding:10px; color:#FFF; background:#882255\">PHASE 6 : COMPUTER VISION </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26a701",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#882255\">Section 6.1 : Classification des Cellules Sanguines</h3>\n",
    "\n",
    "**üìä Datasets utilis√©s :** 7 (`BCCD_github`)\n",
    "\n",
    "**Approches :**\n",
    "- Extraction de features (couleur, forme, texture)\n",
    "- Classification traditionnelle (SVM, Random Forest sur features extraites)\n",
    "- CNN (TensorFlow/Keras) si ressources disponibles\n",
    "\n",
    "**Visualisations :**\n",
    "- Grilles d'images annot√©es par type cellulaire\n",
    "- Confusion matrix (RBC, WBC, Platelets)\n",
    "- Grad-CAM pour visualiser les zones d'attention du CNN\n",
    "- Distribution des classes\n",
    "\n",
    "**‚ùì Questions :**\n",
    "- Peut-on automatiser la classification des types cellulaires ?\n",
    "- Quelle pr√©cision atteint-on avec des m√©thodes classiques vs deep learning ?\n",
    "- Les anomalies cellulaires sont-elles d√©tectables visuellement ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BASE_URL explicitly for fallback crawler in Cell 48\n",
    "BASE_URL = \"https://gayathiri-ravendirane.emi.u-bordeaux.fr/DATA/BCCD_github/\"\n",
    "\n",
    "# Ensure image extensions are available for link filtering\n",
    "if \"IMG_EXTS\" not in globals():\n",
    "    IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65474097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 (Alternative) ‚Äì Lazy HTTP dataset with on-disk cache (no bulk download required)\n",
    "# This cell lets you train directly from URLs. Files are cached on first use.\n",
    "\n",
    "\n",
    "\n",
    "# Optional imports for ML frameworks\n",
    "try:\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "try:\n",
    "    import torchvision.transforms as T\n",
    "except Exception:\n",
    "    T = None\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "except Exception:\n",
    "    Image = None\n",
    "\n",
    "# Reuse session if defined by the crawler cell\n",
    "_http = globals().get(\"SESSION\") or globals().get(\"session\") or requests.Session()\n",
    "\n",
    "CACHE_DIR = Path(\"DATA/BCCD_github_cache\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _cache_path(url: str) -> Path:\n",
    "    h = hashlib.sha1(url.encode()).hexdigest()\n",
    "    ext = os.path.splitext(url)[1] or \".bin\"\n",
    "    return CACHE_DIR / f\"{h}{ext}\"\n",
    "\n",
    "\n",
    "class URLImageDataset:\n",
    "    \"\"\"Generic URL-backed image dataset with local cache.\n",
    "    - urls: list of image URLs\n",
    "    - transform: optional transform (torchvision compatible). If None, returns PIL images.\n",
    "    \"\"\"\n",
    "    def __init__(self, urls, transform=None, session=None):\n",
    "        self.urls = list(urls)\n",
    "        self.transform = transform\n",
    "        self.session = session or _http\n",
    "\n",
    "        if Image is None:\n",
    "            raise RuntimeError(\"Pillow (PIL) is required: pip install pillow\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.urls)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        url = self.urls[idx]\n",
    "        dest = _cache_path(url)\n",
    "        if not dest.exists():\n",
    "            r = self.session.get(url, timeout=60)\n",
    "            r.raise_for_status()\n",
    "            dest.write_bytes(r.content)\n",
    "        img = Image.open(dest).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            return self.transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "# Fallback lightweight crawler if URLs are not yet available\n",
    "\n",
    "def _fallback_list_image_urls(base_url: str):\n",
    "    try:\n",
    "        from bs4 import BeautifulSoup\n",
    "    except Exception:\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"beautifulsoup4\"])  # install bs4\n",
    "        from bs4 import BeautifulSoup\n",
    "\n",
    "    visited = set()\n",
    "    links = []\n",
    "    netloc = urlparse(base_url).netloc\n",
    "    base_path = urlparse(base_url).path.rstrip('/')\n",
    "    IMG_EXTS_LOCAL = set(globals().get(\"IMG_EXTS\", {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}))\n",
    "\n",
    "    def is_ok(u: str) -> bool:\n",
    "        pu = urlparse(u)\n",
    "        return pu.scheme in (\"http\", \"https\") and pu.netloc == netloc and pu.path.startswith(base_path)\n",
    "\n",
    "    def crawl(u: str):\n",
    "        if u in visited:\n",
    "            return\n",
    "        visited.add(u)\n",
    "        try:\n",
    "            resp = _http.get(u, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "        except Exception:\n",
    "            return\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            full = urljoin(u, a[\"href\"])\n",
    "            if not is_ok(full):\n",
    "                continue\n",
    "            if full.endswith(\"/\"):\n",
    "                crawl(full)\n",
    "            else:\n",
    "                ext = Path(urlparse(full).path).suffix.lower()\n",
    "                if ext in IMG_EXTS_LOCAL:\n",
    "                    links.append(full)\n",
    "\n",
    "    crawl(base_url)\n",
    "    # Expose collected links to globals for reuse by other cells\n",
    "    globals()[\"file_links\"] = {\"image\": links}\n",
    "    return links\n",
    "\n",
    "\n",
    "def gather_remote_image_urls():\n",
    "    \"\"\"Use URLs discovered by previous cells; otherwise perform a quick crawl.\"\"\"\n",
    "    if \"paired_stems\" in globals() and \"images_by_stem\" in globals() and globals()[\"paired_stems\"]:\n",
    "        return [globals()[\"images_by_stem\"][s][0] for s in globals()[\"paired_stems\"]]\n",
    "    if \"file_links\" in globals() and globals()[\"file_links\"].get(\"image\"):\n",
    "        return list(globals()[\"file_links\"].get(\"image\", []))\n",
    "    if \"BASE_URL\" in globals():\n",
    "        print(\"No URLs found in memory; performing a quick crawl of BASE_URL...\")\n",
    "        return _fallback_list_image_urls(globals()[\"BASE_URL\"])\n",
    "    raise RuntimeError(\"No URLs found. Run the 6.1 crawl cell first to populate URLs or define BASE_URL.\")\n",
    "\n",
    "\n",
    "# Build URL list from the crawler outputs or fallback crawl\n",
    "urls = gather_remote_image_urls()\n",
    "print(f\"Discovered {len(urls)} image URLs for lazy loading.\")\n",
    "\n",
    "# Example: PyTorch DataLoader (only if torch is available)\n",
    "if torch is not None:\n",
    "    # If torchvision is available, apply a tensor transform; otherwise keep PIL\n",
    "    collate_fn = None\n",
    "    transform = None\n",
    "    if T is not None:\n",
    "        transform = T.Compose([\n",
    "            T.Resize((256, 256)),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "    else:\n",
    "        # Avoid default_collate errors with PIL images by returning the batch as-is\n",
    "        collate_fn = lambda batch: batch\n",
    "\n",
    "    ds = URLImageDataset(urls, transform=transform, session=_http)\n",
    "    try:\n",
    "        dl = DataLoader(ds, batch_size=16, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "        batch = next(iter(dl))  # triggers lazy fetch + cache\n",
    "        if T is not None and hasattr(batch, \"shape\"):\n",
    "            print(\"Loaded a tensor batch with shape:\", batch.shape)\n",
    "        else:\n",
    "            # batch is likely a list of PIL Images\n",
    "            print(f\"Loaded a batch of {len(batch)} items (PIL images or non-tensor).\")\n",
    "    except Exception as e:\n",
    "        print(\"DataLoader demo skipped:\", e)\n",
    "else:\n",
    "    print(\"PyTorch not installed. You can still use URLImageDataset directly to iterate PIL images.\")\n",
    "    print(\"Install with: pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\")\n",
    "\n",
    "# Notes:\n",
    "# - Files are cached to DATA/BCCD_github_cache on first access.\n",
    "# - This avoids bulk download while keeping subsequent epochs fast.\n",
    "# - To clear cache: delete the DATA/BCCD_github_cache directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c34819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 ‚Äì Build annotated patch dataset from XML (RBC/WBC/Platelets)\n",
    "# Parses Pascal VOC XMLs, crops per-object patches, prepares records.\n",
    "\n",
    "# Reuse cache helpers (from lazy-cache cell); define if missing\n",
    "if \"CACHE_DIR\" not in globals():\n",
    "    CACHE_DIR = Path(\"DATA/BCCD_github_cache\").resolve()\n",
    "    CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if \"_http\" not in globals():\n",
    "    import requests\n",
    "    _http = requests.Session()\n",
    "if \"_cache_path\" not in globals():\n",
    "    import os, hashlib\n",
    "    def _cache_path(url: str) -> Path:\n",
    "        h = hashlib.sha1(url.encode()).hexdigest()\n",
    "        ext = os.path.splitext(url)[1] or \".bin\"\n",
    "        return CACHE_DIR / f\"{h}{ext}\"\n",
    "\n",
    "\n",
    "def fetch_to_cache(url: str) -> Path:\n",
    "    p = _cache_path(url)\n",
    "    if not p.exists():\n",
    "        r = _http.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        p.write_bytes(r.content)\n",
    "    return p\n",
    "\n",
    "\n",
    "# Reuse URL mappings from crawler; fallback to BASE_URL crawl if missing\n",
    "def _build_mappings_from_base(base_url: str):\n",
    "    from bs4 import BeautifulSoup\n",
    "    visited = set(); image_urls = []; xml_urls = []\n",
    "    netloc = urlparse(base_url).netloc\n",
    "    base_path = urlparse(base_url).path.rstrip('/')\n",
    "    IMG_EXTS_LOCAL = set(globals().get(\"IMG_EXTS\", {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}))\n",
    "    def is_ok(u: str) -> bool:\n",
    "        pu = urlparse(u)\n",
    "        return pu.scheme in (\"http\", \"https\") and pu.netloc == netloc and pu.path.startswith(base_path)\n",
    "    def crawl(u: str):\n",
    "        if u in visited:\n",
    "            return\n",
    "        visited.add(u)\n",
    "        resp = _http.get(u, timeout=30)\n",
    "        try:\n",
    "            resp.raise_for_status()\n",
    "        except Exception:\n",
    "            return\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            full = urljoin(u, a[\"href\"])\n",
    "            if not is_ok(full):\n",
    "                continue\n",
    "            if full.endswith(\"/\"):\n",
    "                crawl(full)\n",
    "            else:\n",
    "                ext = Path(urlparse(full).path).suffix.lower()\n",
    "                if ext in IMG_EXTS_LOCAL:\n",
    "                    image_urls.append(full)\n",
    "                if ext == \".xml\":\n",
    "                    xml_urls.append(full)\n",
    "    crawl(base_url)\n",
    "    imgs_by_stem = {}\n",
    "    xmls_by_stem = {}\n",
    "    for u in image_urls:\n",
    "        stem = Path(urlparse(u).path).stem\n",
    "        imgs_by_stem.setdefault(stem, []).append(u)\n",
    "    for u in xml_urls:\n",
    "        stem = Path(urlparse(u).path).stem\n",
    "        xmls_by_stem.setdefault(stem, []).append(u)\n",
    "    globals()[\"images_by_stem\"] = imgs_by_stem\n",
    "    globals()[\"xml_by_stem\"] = xmls_by_stem\n",
    "\n",
    "if \"images_by_stem\" not in globals() or \"xml_by_stem\" not in globals():\n",
    "    if \"BASE_URL\" in globals():\n",
    "        _build_mappings_from_base(globals()[\"BASE_URL\"])\n",
    "    else:\n",
    "        raise RuntimeError(\"Define BASE_URL or run the crawl/downloader cell to populate images_by_stem/xml_by_stem.\")\n",
    "\n",
    "LABELS = [\"RBC\", \"WBC\", \"Platelets\"]\n",
    "\n",
    "\n",
    "def parse_voc_xml(xml_path: Path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    objs = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.findtext(\"name\")\n",
    "        bnd = obj.find(\"bndbox\")\n",
    "        if bnd is None:\n",
    "            continue\n",
    "        try:\n",
    "            xmin = int(float(bnd.findtext(\"xmin\")))\n",
    "            ymin = int(float(bnd.findtext(\"ymin\")))\n",
    "            xmax = int(float(bnd.findtext(\"xmax\")))\n",
    "            ymax = int(float(bnd.findtext(\"ymax\")))\n",
    "        except Exception:\n",
    "            continue\n",
    "        objs.append({\"label\": name, \"bbox\": (xmin, ymin, xmax, ymax)})\n",
    "    return objs\n",
    "\n",
    "\n",
    "def crop_patch(img, bbox):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    # Clamp bounds\n",
    "    xmin = max(0, xmin); ymin = max(0, ymin)\n",
    "    xmax = min(img.width, xmax); ymax = min(img.height, ymax)\n",
    "    if xmax <= xmin or ymax <= ymin:\n",
    "        return None\n",
    "    patch = img.crop((xmin, ymin, xmax, ymax))\n",
    "    # Normalize patch size lightly for classical features\n",
    "    return patch.resize((128, 128))\n",
    "\n",
    "\n",
    "patches = []  # list of dicts: {stem, label, patch(PIL.Image), bbox}\n",
    "class_counts = {k: 0 for k in LABELS}\n",
    "\n",
    "paired_stems_sorted = sorted(set(images_by_stem.keys()) & set(xml_by_stem.keys()))\n",
    "print(f\"Paired stems available: {len(paired_stems_sorted)}\")\n",
    "\n",
    "for s in paired_stems_sorted:\n",
    "    img_url = images_by_stem[s][0]\n",
    "    xml_url = xml_by_stem[s][0]\n",
    "    img_path = fetch_to_cache(img_url)\n",
    "    xml_path = fetch_to_cache(xml_url)\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        objs = parse_voc_xml(xml_path)\n",
    "        for o in objs:\n",
    "            label = o[\"label\"]\n",
    "            if label not in LABELS:\n",
    "                continue\n",
    "            patch = crop_patch(img, o[\"bbox\"])\n",
    "            if patch is None:\n",
    "                continue\n",
    "            patches.append({\"stem\": s, \"label\": label, \"patch\": patch, \"bbox\": o[\"bbox\"]})\n",
    "            class_counts[label] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Skip {s}: {e}\")\n",
    "\n",
    "print(\"Patches built:\", len(patches))\n",
    "print(\"Class counts:\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d15564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 ‚Äì Feature extraction and classical ML (SVM, RandomForest)\n",
    "# Extract color, texture, simple stats from patches; train/test split; confusion matrix.\n",
    "\n",
    "\n",
    "# Ensure scikit-learn is available\n",
    "try:\n",
    "    import sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\", \"seaborn\"])\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "if not patches:\n",
    "    raise RuntimeError(\"No patches found. Run the patch-building cell first.\")\n",
    "\n",
    "\n",
    "def patch_features(patch_img):\n",
    "    arr = np.array(patch_img)\n",
    "    # Color histogram (16 bins per channel)\n",
    "    feats = []\n",
    "    for c in range(3):\n",
    "        hist, _ = np.histogram(arr[..., c], bins=16, range=(0, 255), density=True)\n",
    "        feats.extend(hist.tolist())\n",
    "    # Per-channel mean/std\n",
    "    feats.extend(arr.mean(axis=(0, 1)).tolist())\n",
    "    feats.extend(arr.std(axis=(0, 1)).tolist())\n",
    "    # Entropy (grayscale)\n",
    "    gray = (0.299 * arr[..., 0] + 0.587 * arr[..., 1] + 0.114 * arr[..., 2]).astype(np.uint8)\n",
    "    h, _ = np.histogram(gray, bins=32, range=(0, 255), density=True)\n",
    "    h = h + 1e-8\n",
    "    entropy = -np.sum(h * np.log(h))\n",
    "    feats.append(float(entropy))\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "X = np.stack([patch_features(p[\"patch\"]) for p in patches])\n",
    "y_labels = [p[\"label\"] for p in patches]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_labels)\n",
    "\n",
    "print(\"Feature matrix:\", X.shape)\n",
    "print(\"Label distribution:\", Counter(y_labels))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "# SVM\n",
    "svm_clf = SVC(kernel=\"rbf\", C=2.0, gamma=\"scale\")\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_pred = svm_clf.predict(X_test)\n",
    "svm_acc = accuracy_score(y_test, svm_pred)\n",
    "print(\"SVM accuracy:\", svm_acc)\n",
    "\n",
    "# RandomForest\n",
    "rf_clf = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "print(\"RF accuracy:\", rf_acc)\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "cm_svm = confusion_matrix(y_test, svm_pred)\n",
    "cm_rf = confusion_matrix(y_test, rf_pred)\n",
    "sns.heatmap(cm_svm, ax=axes[0], annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "axes[0].set_title(\"SVM Confusion Matrix\")\n",
    "sns.heatmap(cm_rf, ax=axes[1], annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "axes[1].set_title(\"RF Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 ‚Äì Visualizations: annotated grid, class distribution\n",
    "\n",
    "# Grid of sample patches\n",
    "n_show = min(12, len(patches))\n",
    "samples = random.sample(patches, n_show) if len(patches) >= n_show else patches\n",
    "cols = 4\n",
    "rows = int(np.ceil(n_show / cols))\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.axis('off')\n",
    "    if i < n_show:\n",
    "        p = samples[i]\n",
    "        ax.imshow(p[\"patch\"])\n",
    "        ax.set_title(p[\"label\"])\n",
    "plt.suptitle(\"Annotated Patch Grid\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Class distribution\n",
    "counts = Counter([p[\"label\"] for p in patches])\n",
    "labels = list(counts.keys())\n",
    "values = [counts[k] for k in labels]\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=labels, y=values, palette=\"Set2\")\n",
    "plt.title(\"Class Distribution (patch level)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e840ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 ‚Äì Optional CNN (TensorFlow/Keras) and Grad-CAM\n",
    "# Trains a small CNN on patches if TensorFlow is available; shows confusion matrix and Grad-CAM.\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow not installed. Skip CNN. Install with: pip install tensorflow\")\n",
    "    tf = None\n",
    "\n",
    "if tf is not None:\n",
    "    # Prepare data\n",
    "    imgs = np.stack([np.array(p[\"patch\"]) for p in patches])  # (N,128,128,3)\n",
    "    labels = np.array([p[\"label\"] for p in patches])\n",
    "    classes = sorted(set(labels))\n",
    "    class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    y = np.array([class_to_idx[l] for l in labels])\n",
    "\n",
    "    # Train/test split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(imgs, y, test_size=0.25, stratify=y, random_state=42)\n",
    "    # Normalize\n",
    "    X_train = X_train.astype(\"float32\")/255.0\n",
    "    X_test = X_test.astype(\"float32\")/255.0\n",
    "\n",
    "    # Functional model (compatible with Keras 3 Grad-CAM)\n",
    "    inputs = tf.keras.Input(shape=(128,128,3))\n",
    "    x = layers.Conv2D(16, (3,3), activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Conv2D(32, (3,3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    last_conv = layers.Conv2D(64, (3,3), activation='relu', name='last_conv')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(last_conv)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(len(classes), activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='blood_cnn')\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"CNN test accuracy:\", test_acc)\n",
    "\n",
    "    # Confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(\"CNN Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Grad-CAM for one sample\n",
    "    def grad_cam(model, image_array, layer_name='last_conv'):\n",
    "        # Build a model mapping input -> [conv outputs, predictions]\n",
    "        conv_layer = model.get_layer(layer_name)\n",
    "        grad_model = tf.keras.Model(inputs=model.input, outputs=[conv_layer.output, model.output])\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, predictions = grad_model(tf.expand_dims(image_array, axis=0))\n",
    "            top_class = tf.reduce_max(predictions[0])\n",
    "        grads = tape.gradient(top_class, conv_outputs)[0]\n",
    "        conv_outputs = conv_outputs[0]\n",
    "        # Channel-wise importance\n",
    "        weights = tf.reduce_mean(grads, axis=(0,1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, conv_outputs), axis=-1)\n",
    "        cam = tf.maximum(cam, 0)\n",
    "        cam = cam / (tf.reduce_max(cam) + 1e-8)\n",
    "        cam = tf.image.resize(tf.expand_dims(tf.expand_dims(cam, -1), 0), (128,128))[0,...,0].numpy()\n",
    "        return cam\n",
    "\n",
    "    idx0 = 0\n",
    "    img0 = X_test[idx0]\n",
    "    cam0 = grad_cam(model, img0)\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(img0)\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(img0)\n",
    "    plt.imshow(cam0, cmap='jet', alpha=0.35)\n",
    "    plt.title(\"Grad-CAM\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 (viz) ‚Äì Train set predicted labels visualization\n",
    "\n",
    "# Ensure TensorFlow model and train set exist\n",
    "has_tf = 'tf' in globals() and tf is not None\n",
    "has_model = has_tf and 'model' in globals()\n",
    "has_train = 'X_train' in globals() and 'y_train' in globals()\n",
    "if not (has_model and has_train):\n",
    "    print(\"Skip: TensorFlow model or train set not available.\")\n",
    "else:\n",
    "    # Predict on train set\n",
    "    y_pred_train = np.argmax(model.predict(X_train, verbose=0), axis=1)\n",
    "    # Bar plot of predicted label distribution\n",
    "    if 'classes' in globals():\n",
    "        class_names = classes\n",
    "    else:\n",
    "        # Fallback: infer unique indices\n",
    "        class_names = sorted(set(list(y_pred_train)))\n",
    "    counts = {class_names[i]: int((y_pred_train == i).sum()) for i in range(len(class_names))}\n",
    "    labels = list(counts.keys())\n",
    "    values = [counts[k] for k in labels]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(x=labels, y=values, palette=\"Set3\")\n",
    "    plt.title(\"Predicted Label Distribution (Train Set)\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.xlabel(\"class\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Optional: Confusion matrix on train set (true vs predicted)\n",
    "    try:\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.heatmap(cm_train, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Purples')\n",
    "        plt.title(\"CNN Confusion Matrix (Train Set)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Confusion matrix skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acc087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 (viz-test) ‚Äì Test-set clustering and confidence visualizations\n",
    "\n",
    "has_tf = 'tf' in globals() and tf is not None\n",
    "ready = has_tf and 'model' in globals() and 'X_test' in globals() and 'y_test' in globals()\n",
    "if not ready:\n",
    "    print(\"Skip: TensorFlow model or test set not available.\")\n",
    "else:\n",
    "    # Build feature extractor (reuse if available)\n",
    "    try:\n",
    "        feature_model\n",
    "    except NameError:\n",
    "        try:\n",
    "            feature_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "        except Exception as e:\n",
    "            print(\"Fallback to GAP features:\", e)\n",
    "            try:\n",
    "                last_conv_layer = model.get_layer('last_conv')\n",
    "                last_conv_output = last_conv_layer.output\n",
    "            except Exception:\n",
    "                last_conv_output = model.layers[-3].output\n",
    "            gap = tf.keras.layers.GlobalAveragePooling2D()(last_conv_output)\n",
    "            feature_model = tf.keras.Model(inputs=model.input, outputs=gap)\n",
    "    \n",
    "    # Compute test features and predictions\n",
    "    test_feats = feature_model.predict(X_test, verbose=0)\n",
    "    probs_test = model.predict(X_test, verbose=0)\n",
    "    y_pred_test = np.argmax(probs_test, axis=1)\n",
    "    class_names = classes if 'classes' in globals() else [str(i) for i in range(probs_test.shape[1])]\n",
    "    \n",
    "    # PCA clustering (predicted)\n",
    "    try:\n",
    "        from sklearn.decomposition import PCA\n",
    "        Z_pca_test = PCA(n_components=2, random_state=42).fit_transform(test_feats)\n",
    "        dfpca = pd.DataFrame({'x_pca': Z_pca_test[:,0], 'y_pca': Z_pca_test[:,1], 'pred': [class_names[i] for i in y_pred_test], 'true': [class_names[i] if i < len(class_names) else i for i in y_test]})\n",
    "        plt.figure(figsize=(7,5))\n",
    "        sns.scatterplot(data=dfpca, x='x_pca', y='y_pca', hue='pred', alpha=0.7, s=18, palette='Set3', legend='brief')\n",
    "        plt.title('PCA of Test Features colored by Predicted Class')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(7,5))\n",
    "        sns.scatterplot(data=dfpca, x='x_pca', y='y_pca', hue='true', alpha=0.7, s=18, palette='Dark2', legend='brief')\n",
    "        plt.title('PCA colored by True Class (Test)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('PCA unavailable:', e)\n",
    "    \n",
    "    # UMAP clustering (predicted)\n",
    "    try:\n",
    "        import umap\n",
    "        Z_umap_test = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42).fit_transform(test_feats)\n",
    "        dfumap = pd.DataFrame({'x_umap': Z_umap_test[:,0], 'y_umap': Z_umap_test[:,1], 'pred': [class_names[i] for i in y_pred_test], 'true': [class_names[i] if i < len(class_names) else i for i in y_test]})\n",
    "        plt.figure(figsize=(7,5))\n",
    "        sns.scatterplot(data=dfumap, x='x_umap', y='y_umap', hue='pred', alpha=0.7, s=18, palette='Set2', legend='brief')\n",
    "        plt.title('UMAP of Test Features colored by Predicted Class')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(7,5))\n",
    "        sns.scatterplot(data=dfumap, x='x_umap', y='y_umap', hue='true', alpha=0.7, s=18, palette='Dark2', legend='brief')\n",
    "        plt.title('UMAP colored by True Class (Test)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('UMAP unavailable:', e)\n",
    "    \n",
    "    # Probability histograms and calibration (test)\n",
    "    records = []\n",
    "    for i, p in enumerate(probs_test):\n",
    "        pred_idx = y_pred_test[i]\n",
    "        true_idx = y_test[i]\n",
    "        for c in range(len(class_names)):\n",
    "            records.append({'sample': i, 'class': class_names[c], 'prob': float(p[c]), 'pred': class_names[pred_idx], 'true': class_names[true_idx] if true_idx < len(class_names) else str(true_idx)})\n",
    "    dfpt = pd.DataFrame(records)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.kdeplot(data=dfpt, x='prob', hue='pred', common_norm=False, fill=True, alpha=0.35)\n",
    "    plt.title('Softmax Probability Distribution by Predicted Class (Test)')\n",
    "    plt.xlabel('probability')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    conf_t = dfpt[dfpt['class'] == dfpt['pred']]\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    sns.violinplot(data=conf_t, x='pred', y='prob', hue='pred', palette='Set2', inner='quartile', ax=ax)\n",
    "    try:\n",
    "        ax.legend_.remove()\n",
    "    except Exception:\n",
    "        pass\n",
    "    ax.set_title('Confidence (Predicted Class Probability) per Predicted Class ‚Äì Test')\n",
    "    ax.set_xlabel('predicted class'); ax.set_ylabel('probability')\n",
    "    plt.tight_layout(); plt.show()\n",
    "    # Reliability curve\n",
    "    try:\n",
    "        bins = np.linspace(0, 1, 11)\n",
    "        sample_conf = dfpt[dfpt['class'] == dfpt['pred']][['sample','prob','pred']]\n",
    "        sample_true = dfpt.groupby('sample').agg({'true':'first'}).reset_index()\n",
    "        merged = sample_conf.merge(sample_true, on='sample')\n",
    "        merged['correct'] = (merged['pred'] == merged['true']).astype(int)\n",
    "        merged['bin'] = pd.cut(merged['prob'], bins, include_lowest=True)\n",
    "        calib = merged.groupby('bin').agg(conf_mean=('prob','mean'), acc=('correct','mean')).reset_index()\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(calib['conf_mean'], calib['acc'], marker='o')\n",
    "        plt.plot([0,1],[0,1],'--',color='gray',label='ideal')\n",
    "        plt.title('Reliability Curve (Test)')\n",
    "        plt.xlabel('predicted confidence (mean per bin)')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Reliability curve (test) skipped:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe37018",
   "metadata": {},
   "source": [
    "**‚ùì Questions -- R√©ponses :**\n",
    "\n",
    "- **Peut-on automatiser la classification des types cellulaires ?**\n",
    "Oui : m√©thodes classiques (SVM/RF/KNN) pour peu de donn√©es et l‚Äôinterpr√©tabilit√© ; deep learning (CNN) plus robuste et performant avec assez d‚Äôannotations.\n",
    "\n",
    "- **Quelle pr√©cision atteint-on avec des m√©thodes classiques vs deep learning ?**\n",
    "Classiques ~75‚Äì90% selon donn√©es et features ; deep learning ~90‚Äì98% (et >90% mAP) avec dataset bien annot√©, augmentation et bonne validation.\n",
    "\n",
    "- **Les anomalies cellulaires sont-elles d√©tectables visuellement ?**\n",
    "Oui pour beaucoup d‚Äôanomalies morphologiques ; en automatique, pipeline d√©tection + classification, d√©pendant de la qualit√© des images et des annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d0232",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 style=\"padding:10px; color:#FFF; background:#882255\">PHASE 7 : SYNTH√àSE & RECOMMANDATIONS</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e91d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h2 style=\"padding:8px; color:#FFF; background:#E3CFC6\">Tableau R√©capitulatif des Couplages Datasets</h2>\n",
    "\n",
    "| **Phase** | **Section** | **Datasets** | **Type d'analyse** | **Flow** | **Evolution** |\n",
    "|---|---|---|---|---|---|\n",
    "| Exploration | 1.1 | 1, 2, 3, 5 | Descriptive, nettoyage | Clean & Structure | ‚úÖ|\n",
    "| D√©mographie | 2.1 | 5 | Descriptive, visualisation | Analyze & Visualize | |\n",
    "| Pression art√©rielle | 2.2 | 2 | Descriptive, distribution | Analyze & Visualize | ‚úÖ|\n",
    "| Biomarqueurs | 2.3 | 3 | Corr√©lations, profils | Analyze & Visualize | |\n",
    "| G√©ographie mondiale | 3.1 | 4 | Cartographie, comparaisons | Analyze & Visualize | |\n",
    "| G√©ographie Inde | 3.2 | 6 | Cartographie, √©pid√©miologie | Analyze & Visualize | |\n",
    "| Risques hypertension | 4.1 | 1 + 2 + 3 | Corr√©lations, tests statistiques | Analyze & Model | ‚úÖ|\n",
    "| Pr√©diction don | 4.2 | 1 + 2 + 3 + 5 | Classification, r√©gression | Model | |\n",
    "| Classification profils √† risque | 5.1 | 1 + 2 + 3 + 5 | Classification | Model & Analyze | |\n",
    "| R√©gression pression art√©rielle | 5.2 | 2 + 3 | R√©gression | Model | |\n",
    "| Segmentation | 5.3 | 5 | Clustering | Model & Analyze | |\n",
    "| Vision | 6.1 | 7 | Classification d'images | Model & Visualize | ‚úÖ|\n",
    "| Dashboard | 7.0 | Tous | Synth√®se interactive | Visualize | ‚úÖ|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9561bdf",
   "metadata": {},
   "source": [
    "<div style=\"padding:8px; margin:0px -20px; color:#FFF; background:#2E1A24; text-align:right\">‚óè ‚óè ‚óè </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outinfo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
